{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import duckdb, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Point to your unzipped PL state folder (contains geo + data CSVs)\n",
    "DATA_DIR = Path(\"/path/to/your/unzipped/state_folder\")  # e.g., Path(\"tx_pl\")\n",
    "\n",
    "# Where to save outputs\n",
    "OUT_DIR = Path(\"data/processed\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "con = duckdb.connect((OUT_DIR / \"pl_state.duckdb\").as_posix())\n",
    "print(\"DuckDB:\", duckdb.__version__)\n"
   ],
   "id": "3f8ee26624b8c0f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Heuristics to find GEO vs DATA files\n",
    "geo_candidates  = sorted([p for p in DATA_DIR.glob(\"**/*.csv\") if \"geo\" in p.name.lower()])\n",
    "data_candidates = sorted([p for p in DATA_DIR.glob(\"**/*.csv\") if \"geo\" not in p.name.lower()])\n",
    "\n",
    "assert geo_candidates, \"Couldn't find a GEO header CSV (name usually contains 'geo').\"\n",
    "assert data_candidates, \"Couldn't find any data CSVs.\"\n",
    "\n",
    "GEO_FILE  = geo_candidates[0]        # typically only one geo file\n",
    "DATA_FILES = [p.as_posix() for p in data_candidates]\n",
    "\n",
    "print(\"GEO:\", GEO_FILE.name)\n",
    "print(\"DATA files:\", len(DATA_FILES))\n"
   ],
   "id": "a1f00cb3a59c7bf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read GEO header: keep everything as VARCHAR first (safer)\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE st_geo AS\n",
    "SELECT * FROM read_csv_auto(?, ALL_VARCHAR=TRUE);\n",
    "\"\"\", [GEO_FILE.as_posix()])\n",
    "\n",
    "# Read ALL data CSVs into one wide table\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE st_data AS\n",
    "SELECT * FROM read_csv_auto(?, ALL_VARCHAR=TRUE)\n",
    "\"\"\", [DATA_FILES])\n"
   ],
   "id": "7b747b3c2d375969"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "geo_cols  = {r[0] for r in con.execute(\"PRAGMA table_info('st_geo')\").fetchall()}\n",
    "data_cols = {r[0] for r in con.execute(\"PRAGMA table_info('st_data')\").fetchall()}\n",
    "\n",
    "# GEO essentials\n",
    "must_geo = {\"SUMLEV\",\"LOGRECNO\"}\n",
    "assert must_geo.issubset(geo_cols), f\"GEO file missing required columns: {must_geo - geo_cols}\"\n",
    "\n",
    "# Determine variable naming scheme\n",
    "modern_needed = {\"P1_001N\",\"P3_001N\",\"P4_003N\"}\n",
    "legacy_needed = {\"P0010001\",\"P0030001\",\"P0040003\"}\n",
    "\n",
    "if modern_needed.issubset(data_cols):\n",
    "    scheme = \"modern\"\n",
    "elif legacy_needed.issubset(data_cols):\n",
    "    scheme = \"legacy\"\n",
    "else:\n",
    "    raise RuntimeError(\"Couldn't detect PL column naming scheme (modern vs legacy). Inspect st_data columns.\")\n",
    "print(\"Detected scheme:\", scheme)\n"
   ],
   "id": "5c80de26bee1762b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Map variables for both schemes\n",
    "if scheme == \"modern\":\n",
    "    P1_TOTAL      = \"P1_001N\"\n",
    "    P3_VAP        = \"P3_001N\"\n",
    "    RACE_PARTS    = [\"P3_003N\",\"P3_004N\",\"P3_005N\",\"P3_006N\",\"P3_007N\",\"P3_008N\",\"P3_009N\"]  # White, Black, AIAN, Asian, NHPI, Other, 2+\n",
    "    HISP_PARTS    = [\"P4_002N\",\"P4_003N\"]  # Hispanic VAP, Not Hispanic VAP\n",
    "else:\n",
    "    # Legacy naming (P0010001 etc). These are the standard PL 2020 legacy codes.\n",
    "    P1_TOTAL      = \"P0010001\"\n",
    "    P3_VAP        = \"P0030001\"\n",
    "    RACE_PARTS    = [\"P0030003\",\"P0030004\",\"P0030005\",\"P0030006\",\"P0030007\",\"P0030008\",\"P0030009\"]\n",
    "    HISP_PARTS    = [\"P0040002\",\"P0040003\"]\n",
    "\n",
    "# Create a narrow data view with only the columns we need (+ LOGRECNO)\n",
    "need_cols = [P1_TOTAL, P3_VAP] + RACE_PARTS + HISP_PARTS + [\"LOGRECNO\"]\n",
    "sel_list  = \", \".join(need_cols)\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE st_data_narrow AS\n",
    "SELECT {sel_list}\n",
    "FROM st_data\n",
    "\"\"\")\n",
    "\n",
    "# Filter GEO to blocks (SUMLEV = '101'); keep useful geo fields if available\n",
    "# GEO files vary by state/package; try to grab standard IDs when present.\n",
    "geo_select = \"\"\"\n",
    "  g.LOGRECNO,\n",
    "  g.SUMLEV,\n",
    "  COALESCE(g.GEOID, g.GEOID20, g.BLKIDFP, g.BLOCKID) AS GEOID_any,\n",
    "  COALESCE(g.STATE, g.STATEFP, g.STATEFP20)   AS statefp,\n",
    "  COALESCE(g.COUNTY, g.COUNTYFP, g.COUNTYFP20) AS countyfp,\n",
    "  COALESCE(g.TRACT, g.TRACTCE, g.TRACTCE20)  AS tractce,\n",
    "  COALESCE(g.BLOCK, g.BLOCKCE, g.BLOCKCE20)  AS blockce\n",
    "\"\"\"\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE st_geo_blocks AS\n",
    "SELECT {geo_select}\n",
    "FROM st_geo g\n",
    "WHERE g.SUMLEV = '101'\n",
    "\"\"\")\n",
    "\n",
    "# Join on LOGRECNO to get block-level records with PL variables\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE pl_blocks_raw AS\n",
    "SELECT\n",
    "  gb.LOGRECNO,\n",
    "  gb.GEOID_any,\n",
    "  gb.statefp, gb.countyfp, gb.tractce, gb.blockce,\n",
    "  d.*\n",
    "FROM st_geo_blocks gb\n",
    "JOIN st_data_narrow d USING (LOGRECNO)\n",
    "\"\"\")\n"
   ],
   "id": "b62492edfacfb28d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build cast expressions\n",
    "def cast_list(cols, as_type):\n",
    "    return \",\\n       \".join([f\"CAST({c} AS {as_type}) AS {c}\" for c in cols])\n",
    "\n",
    "race_casts = cast_list(RACE_PARTS, \"BIGINT\")\n",
    "hisp_casts = cast_list(HISP_PARTS, \"BIGINT\")\n",
    "\n",
    "share_exprs = \",\\n       \".join([\n",
    "    f\"CAST({c} AS DOUBLE)/NULLIF(CAST({P3_VAP} AS DOUBLE),0) AS {c}_share_vap\"\n",
    "    for c in (RACE_PARTS + HISP_PARTS)\n",
    "])\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS census_demographics_clean;\")\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE census_demographics_clean AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    -- standardized GEOID20 (pick GEOID if present, otherwise build from parts)\n",
    "    COALESCE(\n",
    "      NULLIF(TRIM(GEOID_any), ''),\n",
    "      statefp || countyfp || tractce || blockce\n",
    "    )                                        AS GEOID20,\n",
    "    statefp, countyfp, tractce, blockce,\n",
    "    -- counts (casted)\n",
    "    CAST({P1_TOTAL} AS BIGINT) AS P1_001N,\n",
    "    CAST({P3_VAP}   AS BIGINT) AS P3_001N,\n",
    "    {race_casts},\n",
    "    {hisp_casts}\n",
    "  FROM pl_blocks_raw\n",
    "),\n",
    "shares AS (\n",
    "  SELECT\n",
    "    *,\n",
    "    {share_exprs}\n",
    "  FROM base\n",
    ")\n",
    "SELECT * FROM shares;\n",
    "\"\"\")\n",
    "\n",
    "print(con.execute(\"SELECT COUNT(*) rows, COUNT(DISTINCT GEOID20) uniq FROM census_demographics_clean;\").fetchdf())\n"
   ],
   "id": "b233122c169ae354"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Unique GEOID20\n",
    "print(con.execute(\"\"\"\n",
    "SELECT COUNT(*) rows, COUNT(DISTINCT GEOID20) uniq\n",
    "FROM census_demographics_clean;\n",
    "\"\"\").fetchdf())\n",
    "\n",
    "# GEOID20 looks like 15 digits\n",
    "print(con.execute(\"\"\"\n",
    "SELECT SUM(LENGTH(GEOID20)=15 AND GEOID20 ~ '^[0-9]+$') AS ok_15,\n",
    "       COUNT(*) AS rows\n",
    "FROM census_demographics_clean;\n",
    "\"\"\").fetchdf())\n",
    "\n",
    "# Non-negative counts; VAP <= total pop\n",
    "print(con.execute(f\"\"\"\n",
    "SELECT\n",
    "  SUM(({P1_TOTAL.replace('P0010001','P1_001N').replace('P1_001N','P1_001N')} < 0) OR (P3_001N < 0)) AS neg_pop,\n",
    "  SUM(({ ' OR '.join([c+' < 0' for c in RACE_PARTS]) })) AS neg_race,\n",
    "  SUM(({ ' OR '.join([c+' < 0' for c in HISP_PARTS]) }))  AS neg_hisp,\n",
    "  AVG( (P3_001N <= P1_001N)::DOUBLE ) AS pct_vap_le_total\n",
    "FROM census_demographics_clean;\n",
    "\"\"\").fetchdf())\n"
   ],
   "id": "a1f438ec50f04279"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parquet_path = OUT_DIR / \"census_demographics.parquet\"\n",
    "csv_path     = OUT_DIR / \"census_demographics.csv.gz\"\n",
    "\n",
    "con.execute(f\"COPY census_demographics_clean TO '{parquet_path.as_posix()}' (FORMAT PARQUET);\")\n",
    "con.execute(f\"COPY census_demographics_clean TO '{csv_path.as_posix()}' WITH (HEADER, DELIM ',', COMPRESSION 'gzip');\")\n",
    "print(\"Saved:\", parquet_path, \"and\", csv_path)\n"
   ],
   "id": "b85953615b8abf85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
