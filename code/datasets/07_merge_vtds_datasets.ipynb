{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Needed Libraries and Filepaths",
   "id": "26662965072d83dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:28.552031Z",
     "start_time": "2025-09-20T17:28:28.223081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from texas_gerrymandering_hb4.config import INTERIM_DATA_DIR, CLEAN_ELECTION_RESULTS, CLEAN_VTD_GEO\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import pyarrow"
   ],
   "id": "3a5e6d7e583a6a34",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-09-20 10:28:28.243\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mtexas_gerrymandering_hb4.config\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m11\u001B[0m - \u001B[1mPROJ_ROOT path is: /home/aimlexpert/Documents/GitHub/texas-gerrymandering-HB4\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:28.562405Z",
     "start_time": "2025-09-20T17:28:28.560427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inner_parquet = INTERIM_DATA_DIR/\"inner_join_vtds.parquet\"\n",
    "left_parquet = INTERIM_DATA_DIR/\"left_join_vtds.parquet\""
   ],
   "id": "393b9d26bb677b02",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Helper Functions",
   "id": "d5a2785a7c140b79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:28.611855Z",
     "start_time": "2025-09-20T17:28:28.606719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------ Helpers ------------------------\n",
    "def to_str_or_none(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, float) and math.isnan(x):\n",
    "        return None\n",
    "    return str(x)\n",
    "\n",
    "def clean_digits(x):\n",
    "    s = to_str_or_none(x)\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\D\", \"\", s)   # keep digits only\n",
    "    return s if s else None\n",
    "\n",
    "def build_cntyvtd_norm_from_fips_vtd(fips, vtd):\n",
    "    f = clean_digits(fips)\n",
    "    v = clean_digits(vtd)\n",
    "    if not f or not v:\n",
    "        return None\n",
    "    return f.zfill(3) + v.zfill(4)   # CCC + VVVV\n",
    "\n",
    "def normalize_cntyvtd_string(raw, fallback_fips=None):\n",
    "    \"\"\"\n",
    "    Normalize any CNTYVTD-like string into 7-digit CCCVVVV.\n",
    "    If it's 7 digits already, keep it.\n",
    "    If it's 6 => assume 3+3 and pad VTD to 4.\n",
    "    If it's 5 => often CC+VVV; upgrade to CCC+VVVV by zfilling.\n",
    "    If it's 4 => VTD only -> need fallback_fips to build.\n",
    "    Otherwise try to coerce by padding left.\n",
    "    \"\"\"\n",
    "    s = clean_digits(raw)\n",
    "    if s is None:\n",
    "        # try fallback with vtd-only branch below by passing raw again\n",
    "        if fallback_fips is not None:\n",
    "            return build_cntyvtd_norm_from_fips_vtd(fallback_fips, raw)\n",
    "        return None\n",
    "\n",
    "    if len(s) == 7:\n",
    "        return s\n",
    "    if len(s) == 6:\n",
    "        return s[:3] + s[3:].zfill(4)\n",
    "    if len(s) == 5:\n",
    "        return s[:2].zfill(3) + s[2:].zfill(4)\n",
    "    if len(s) == 4:\n",
    "        if fallback_fips is not None:\n",
    "            return build_cntyvtd_norm_from_fips_vtd(fallback_fips, s)\n",
    "        return None\n",
    "    # odd lengths: pad to 7 on the left as last resort\n",
    "    return s.zfill(7)\n",
    "\n",
    "def freq_len(series):\n",
    "    return series.dropna().astype(str).str.len().value_counts().sort_index()"
   ],
   "id": "d5560508726db94e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "21601bfccb6431a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:28.672750Z",
     "start_time": "2025-09-20T17:28:28.657970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Election Results\n",
    "df = pd.read_csv(CLEAN_ELECTION_RESULTS)"
   ],
   "id": "5f480c858e868804",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:28.895773Z",
     "start_time": "2025-09-20T17:28:28.709888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VTD Geospatial Data\n",
    "gdf = gpd.read_parquet(CLEAN_VTD_GEO)"
   ],
   "id": "10a104184487bdb2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:28.903040Z",
     "start_time": "2025-09-20T17:28:28.900799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------ Column cleanup ------------------------\n",
    "gdf.columns = [c.strip().lower() for c in gdf.columns]\n",
    "df.columns  = [c.strip().lower() for c in df.columns]\n"
   ],
   "id": "99431504a89ddd20",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:28.979845Z",
     "start_time": "2025-09-20T17:28:28.951549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------ Build normalized keys ------------------------\n",
    "geo_fips_candidates = [\"cnty\", \"cntykey\", \"county_fips\", \"countyfp\", \"fips\"]\n",
    "geo_vtd_candidates = [\"vtd\", \"vtdkey\", \"precinct\", \"pct\"]\n",
    "geo_fips_col = next((c for c in geo_fips_candidates if c in gdf.columns), None)\n",
    "geo_vtd_col = next((c for c in geo_vtd_candidates if c in gdf.columns), None)\n",
    "\n",
    "if geo_fips_col and geo_vtd_col:\n",
    "    gdf[\"cntyvtd_norm\"] = [\n",
    "        build_cntyvtd_norm_from_fips_vtd(f, v)\n",
    "        for f, v in zip(gdf[geo_fips_col], gdf[geo_vtd_col])\n",
    "    ]\n",
    "elif \"cntyvtd\" in gdf.columns:\n",
    "    fallback = gdf[geo_fips_col] if geo_fips_col else None\n",
    "    fallback_iter = fallback if fallback is not None else [None] * len(gdf)\n",
    "    gdf[\"cntyvtd_norm\"] = [\n",
    "        normalize_cntyvtd_string(val, fallback_fips=(None if pd.isna(fb) else fb))\n",
    "        for val, fb in zip(gdf[\"cntyvtd\"], fallback_iter)\n",
    "    ]\n",
    "else:\n",
    "    raise KeyError(\"Geo file missing both (cnty,vtd) and cntyvtd columns needed to build the join key.\")\n"
   ],
   "id": "7ee30a1a93ceac0b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:29.034029Z",
     "start_time": "2025-09-20T17:28:29.007597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RESULTS side: build from fips + vtd whenever possible (most reliable)\n",
    "res_fips_candidates = [\"fips\", \"county_fips\", \"countyfp\", \"cnty\"]\n",
    "res_vtd_candidates = [\"vtd\", \"vtdkey\", \"precinct\", \"pct\"]\n",
    "res_fips_col = next((c for c in res_fips_candidates if c in df.columns), None)\n",
    "res_vtd_col = next((c for c in res_vtd_candidates if c in df.columns), None)\n",
    "\n",
    "if res_fips_col and res_vtd_col:\n",
    "    df[\"cntyvtd_norm\"] = [\n",
    "        build_cntyvtd_norm_from_fips_vtd(f, v)\n",
    "        for f, v in zip(df[res_fips_col], df[res_vtd_col])\n",
    "    ]\n",
    "elif \"cntyvtd\" in df.columns:\n",
    "    fallback = df[res_fips_col] if res_fips_col else None\n",
    "    fallback_iter = fallback if fallback is not None else [None] * len(df)\n",
    "    df[\"cntyvtd_norm\"] = [\n",
    "        normalize_cntyvtd_string(val, fallback_fips=(None if pd.isna(fb) else fb))\n",
    "        for val, fb in zip(df[\"cntyvtd\"], fallback_iter)\n",
    "    ]\n",
    "else:\n",
    "    raise KeyError(\"Results file missing both (fips,vtd) and cntyvtd. Need one of those to build the key.\")\n"
   ],
   "id": "733372526d472509",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:29.096541Z",
     "start_time": "2025-09-20T17:28:29.081726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------ Diagnostics ------------------------\n",
    "print(\"Geo key length distribution:\\n\", freq_len(gdf[\"cntyvtd_norm\"]))\n",
    "print(\"Res key length distribution:\\n\", freq_len(df[\"cntyvtd_norm\"]))\n",
    "\n",
    "geo_null = gdf[\"cntyvtd_norm\"].isna().sum()\n",
    "res_null = df[\"cntyvtd_norm\"].isna().sum()\n",
    "print(f\"Null geo keys: {geo_null}/{len(gdf)}\")\n",
    "print(f\"Null res keys: {res_null}/{len(df)}\")\n",
    "\n",
    "geo_keys = set(gdf[\"cntyvtd_norm\"].dropna().unique())\n",
    "res_keys = set(df[\"cntyvtd_norm\"].dropna().unique())\n",
    "\n",
    "only_in_geo = sorted(geo_keys - res_keys)[:15]\n",
    "only_in_res = sorted(res_keys - geo_keys)[:15]\n",
    "print(\"Only-in-geo sample:\", only_in_geo)\n",
    "print(\"Only-in-results sample:\", only_in_res)"
   ],
   "id": "1e1191df0eadccad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo key length distribution:\n",
      " cntyvtd_norm\n",
      "7    9712\n",
      "Name: count, dtype: int64\n",
      "Res key length distribution:\n",
      " cntyvtd_norm\n",
      "7    9712\n",
      "Name: count, dtype: int64\n",
      "Null geo keys: 0/9712\n",
      "Null res keys: 0/9712\n",
      "Only-in-geo sample: []\n",
      "Only-in-results sample: []\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:29.159740Z",
     "start_time": "2025-09-20T17:28:29.144283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If duplicates in results, aggregate sensibly\n",
    "vote_cols = [c for c in [\"dem_votes\",\"rep_votes\",\"third_party_votes\",\"total_votes\",\"dem_share\"] if c in df.columns]\n",
    "if df[\"cntyvtd_norm\"].duplicated().any():\n",
    "    agg_map = {c: \"sum\" for c in vote_cols if c != \"dem_share\"}\n",
    "    df_agg = df.groupby(\"cntyvtd_norm\", as_index=False).agg(agg_map)\n",
    "    if {\"dem_votes\",\"total_votes\"}.issubset(df_agg.columns):\n",
    "        df_agg[\"dem_share\"] = df_agg[\"dem_votes\"] / df_agg[\"total_votes\"].replace({0: pd.NA})\n",
    "else:\n",
    "    df_agg = df.copy()"
   ],
   "id": "22121b9f2dcec39",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:29.211324Z",
     "start_time": "2025-09-20T17:28:29.197209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------ Joins ------------------------\n",
    "inner = gdf.merge(df_agg, on=\"cntyvtd_norm\", how=\"inner\")\n",
    "left  = gdf.merge(df_agg, on=\"cntyvtd_norm\", how=\"left\")\n",
    "\n",
    "# Fill vote NaNs with 0 on left; recompute share if possible\n",
    "for c in vote_cols:\n",
    "    if c in left.columns and c != \"dem_share\":\n",
    "        left[c] = left[c].fillna(0).astype(\"Int64\")\n",
    "if {\"dem_votes\",\"total_votes\"}.issubset(left.columns):\n",
    "    left[\"dem_share\"] = (left[\"dem_votes\"].astype(\"Int64\").fillna(0) /\n",
    "                         left[\"total_votes\"].replace({0: pd.NA}))\n",
    "\n",
    "print(f\"Rows — geo: {len(gdf)}, results(agg): {len(df_agg)}, inner: {len(inner)}, left: {len(left)}\")"
   ],
   "id": "2d6b29e48d3c120",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows — geo: 9712, results(agg): 9389, inner: 9712, left: 9712\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T17:28:29.682934Z",
     "start_time": "2025-09-20T17:28:29.247778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Exports\n",
    "#inner.drop(columns=[\"geometry\"], errors=\"ignore\").to_csv(INTERIM_DATA_DIR/\"inner_join_vtds.csv\", index=False)\n",
    "#left.drop(columns=[\"geometry\"], errors=\"ignore\").to_csv(INTERIM_DATA_DIR/\"left_join_vtds.csv\", index=False)\n",
    "\n",
    "# GeoPackage + GeoParquet\n",
    "#inner.to_file(INTERIM_DATA_DIR/\"vtd_merged.gpkg\", layer=\"vtd_merged_inner\", driver=\"GPKG\")\n",
    "#left.to_file(INTERIM_DATA_DIR/\"vtd_merged.gpkg\", layer=\"vtd_merged_left\", driver=\"GPKG\")\n",
    "\n",
    "# If parquet environment is ready:\n",
    "inner.to_parquet(inner_parquet)\n",
    "left.to_parquet(left_parquet)"
   ],
   "id": "e178261e97371b34",
   "outputs": [],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
