{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T05:35:16.521515Z",
     "start_time": "2025-09-28T05:35:16.443546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Texas Districts: Racial Demographics Maps (GeoParquet version) ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from texas_gerrymandering_hb4.config import FINAL_CSV, CLEAN_DISTRICTS_PARQUET\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# If your percentage columns have different names, set them here:\n",
    "# Any of these can be None, and we'll try to auto-detect by name.\n",
    "RACE_COLS = {\n",
    "    \"White (%)\":    None,  # e.g., \"pct_white\", \"white_share\", \"white_percent\"\n",
    "    \"Black (%)\":    None,  # e.g., \"pct_black\", \"black_share\"\n",
    "    \"Hispanic (%)\": None,  # e.g., \"pct_hispanic\", \"hispanic_share\"\n",
    "    \"Asian (%)\":    None,  # e.g., \"pct_asian\", \"asian_share\"\n",
    "}\n",
    "\n",
    "# Preferred join keys (must exist in BOTH files)\n",
    "PREFERRED_KEYS = [\n",
    "    \"district_id\",\"DISTRICT_ID\",\"district\",\"DISTRICT\",\n",
    "    \"GEOID\",\"GEOID20\",\"CD\",\"CD118FP\",\"NAME\"\n",
    "]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def best_join_key(left: pd.DataFrame, right: pd.DataFrame):\n",
    "    \"\"\"Find a reasonable join key shared by left and right dataframes.\"\"\"\n",
    "    def candidates(df):\n",
    "        return [c for c in df.columns if c != \"geometry\" and df[c].dtype.kind in \"iufOSU\"]\n",
    "\n",
    "    lc, rc = candidates(left), candidates(right)\n",
    "\n",
    "    # 1) Try preferred exact matches\n",
    "    for k in PREFERRED_KEYS:\n",
    "        if k in lc and k in rc:\n",
    "            return k, k\n",
    "\n",
    "    # 2) Case-insensitive matches with max value overlap\n",
    "    lmap = {c.lower(): c for c in lc}\n",
    "    rmap = {c.lower(): c for c in rc}\n",
    "    common = set(lmap) & set(rmap)\n",
    "    if common:\n",
    "        best = None; best_overlap = -1\n",
    "        for lo in common:\n",
    "            lcol, rcol = lmap[lo], rmap[lo]\n",
    "            lvals = set(left[lcol].astype(str).unique())\n",
    "            rvals = set(right[rcol].astype(str).unique())\n",
    "            ov = len(lvals & rvals)\n",
    "            if ov > best_overlap:\n",
    "                best_overlap, best = ov, (lcol, rcol)\n",
    "        if best:\n",
    "            return best\n",
    "\n",
    "    raise ValueError(\"Couldn't find a common join key. Please set it manually.\")\n",
    "\n",
    "def find_pct_col(df: pd.DataFrame, tokens):\n",
    "    \"\"\"\n",
    "    Find a column that looks like the requested group’s PERCENT/SHARE.\n",
    "    tokens: list of regex strings, e.g., [r'hisp', r'latino']\n",
    "    Preference: names containing 'pct'/'percent'/'share'.\n",
    "    \"\"\"\n",
    "    # 1) percentage-like\n",
    "    for c in df.columns:\n",
    "        name = c.lower()\n",
    "        if any(t in name for t in [\"pct\",\"percent\",\"share\"]):\n",
    "            if any(re.search(t, name) for t in tokens):\n",
    "                return c\n",
    "    # 2) fallback: any name with tokens (may be counts)\n",
    "    for c in df.columns:\n",
    "        name = c.lower()\n",
    "        if any(re.search(t, name) for t in tokens):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def ensure_percent(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"If max≤1.01, treat as proportion and convert to percent.\"\"\"\n",
    "    try:\n",
    "        mx = float(np.nanmax(series.astype(float).values))\n",
    "    except Exception:\n",
    "        return series\n",
    "    if mx <= 1.01:\n",
    "        return series * 100.0\n",
    "    return series\n",
    "\n",
    "def pretty_bounds(ax, gdf):\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# ---------- load and join ----------\n",
    "tab = pd.read_csv(FINAL_CSV)\n",
    "geo = gpd.read_parquet(CLEAN_DISTRICTS_PARQUET)  # <-- GeoParquet read\n",
    "\n",
    "# If your GeoParquet stored the ID as index, bring it back:\n",
    "if geo.index.name and geo.index.name not in geo.columns:\n",
    "    geo = geo.reset_index()\n",
    "\n",
    "# Find a join key and merge (override here if you prefer explicit keys)\n",
    "left_key, right_key = best_join_key(tab, geo)\n",
    "merged = geo.merge(tab, left_on=right_key, right_on=left_key, how=\"left\")\n",
    "\n",
    "# Reproject for nicer TX look (ignore if CRS missing or reprojection fails)\n",
    "if merged.crs:\n",
    "    try:\n",
    "        merged = merged.to_crs(3083)  # NAD83 / Texas Centric Albers Equal Area\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Auto-detect race columns if any are None\n",
    "auto_specs = {\n",
    "    \"White (%)\":    [r\"white\", r\"\\bnhw\\b\", r\"anglo\"],\n",
    "    \"Black (%)\":    [r\"black\", r\"african\"],\n",
    "    \"Hispanic (%)\": [r\"hisp\", r\"latino\", r\"latinx\"],\n",
    "    \"Asian (%)\":    [r\"asian\"],\n",
    "}\n",
    "for label, spec in auto_specs.items():\n",
    "    if RACE_COLS.get(label) is None:\n",
    "        RACE_COLS[label] = find_pct_col(merged, spec)\n",
    "\n",
    "# Validate we found everything\n",
    "missing = [lbl for lbl, col in RACE_COLS.items() if col is None or col not in merged.columns]\n",
    "if missing:\n",
    "    raise KeyError(\n",
    "        \"Could not find these columns: \"\n",
    "        + \", \".join(missing)\n",
    "        + \"\\nPlease edit RACE_COLS to your exact column names.\"\n",
    "    )\n",
    "\n",
    "# Ensure percentages and stage plotting columns with pretty labels\n",
    "for lbl, col in list(RACE_COLS.items()):\n",
    "    merged[lbl] = ensure_percent(merged[col])\n",
    "\n",
    "# ---------- 2×2 small multiples ----------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, (label, _) in zip(axes, RACE_COLS.items()):\n",
    "    merged.plot(\n",
    "        column=label,\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        cmap=\"OrRd\",\n",
    "        missing_kwds={\"color\": \"#f0f0f0\", \"hatch\": \"///\", \"label\": \"No data\"},\n",
    "    )\n",
    "    ax.set_title(label, fontsize=14)\n",
    "    pretty_bounds(ax, merged)\n",
    "\n",
    "fig.suptitle(\"Texas Congressional Districts — Racial Demographics\", fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tx_districts_race_small_multiples.png\", dpi=220, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved small multiples:\", \"tx_districts_race_small_multiples.png\")\n",
    "\n",
    "# ---------- Individual PNGs per group ----------\n",
    "for label in RACE_COLS.keys():\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    merged.plot(\n",
    "        column=label,\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        cmap=\"OrRd\",\n",
    "        missing_kwds={\"color\": \"#f0f0f0\", \"hatch\": \"///\", \"label\": \"No data\"},\n",
    "    )\n",
    "    ax.set_title(f\"Texas Congressional Districts — {label}\", fontsize=16)\n",
    "    pretty_bounds(ax, merged)\n",
    "    out = f\"tx_districts_{re.sub('[^a-z]+','_', label.lower()).strip('_')}.png\"\n",
    "    plt.savefig(out, dpi=220, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", out)\n"
   ],
   "id": "ed91128797df5e08",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow.parquet'. pyarrow is required for Parquet support.  \"\n        \"Use pip or conda to install pyarrow.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 99\u001B[39m\n\u001B[32m     97\u001B[39m \u001B[38;5;66;03m# ---------- load and join ----------\u001B[39;00m\n\u001B[32m     98\u001B[39m tab = pd.read_csv(FINAL_CSV)\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m geo = \u001B[43mgpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCLEAN_DISTRICTS_PARQUET\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# <-- GeoParquet read\u001B[39;00m\n\u001B[32m    101\u001B[39m \u001B[38;5;66;03m# If your GeoParquet stored the ID as index, bring it back:\u001B[39;00m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m geo.index.name \u001B[38;5;129;01mand\u001B[39;00m geo.index.name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m geo.columns:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/texas-gerrymandering-hb4/.venv/lib/python3.12/site-packages/geopandas/io/arrow.py:748\u001B[39m, in \u001B[36m_read_parquet\u001B[39m\u001B[34m(path, columns, storage_options, bbox, to_pandas_kwargs, **kwargs)\u001B[39m\n\u001B[32m    668\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_read_parquet\u001B[39m(\n\u001B[32m    669\u001B[39m     path,\n\u001B[32m    670\u001B[39m     columns=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    674\u001B[39m     **kwargs,\n\u001B[32m    675\u001B[39m ):\n\u001B[32m    676\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    677\u001B[39m \u001B[33;03m    Load a Parquet object from the file path, returning a GeoDataFrame.\u001B[39;00m\n\u001B[32m    678\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    746\u001B[39m \u001B[33;03m    ... )  # doctest: +SKIP\u001B[39;00m\n\u001B[32m    747\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m748\u001B[39m     parquet = \u001B[43mimport_optional_dependency\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    749\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpyarrow.parquet\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpyarrow is required for Parquet support.\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m    750\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    751\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgeopandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mio\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_pyarrow_hotfix\u001B[39;00m  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[32m    753\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pandas-dev/pandas/pull/41194): see if pandas\u001B[39;00m\n\u001B[32m    754\u001B[39m     \u001B[38;5;66;03m# adds filesystem as a keyword and match that.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/GitHub/texas-gerrymandering-hb4/.venv/lib/python3.12/site-packages/geopandas/_compat.py:59\u001B[39m, in \u001B[36mimport_optional_dependency\u001B[39m\u001B[34m(name, extra)\u001B[39m\n\u001B[32m     56\u001B[39m     module = importlib.import_module(name)\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m module\n",
      "\u001B[31mImportError\u001B[39m: Missing optional dependency 'pyarrow.parquet'. pyarrow is required for Parquet support.  \"\n        \"Use pip or conda to install pyarrow.parquet."
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
