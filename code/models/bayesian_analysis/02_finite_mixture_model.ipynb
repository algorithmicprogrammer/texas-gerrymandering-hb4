{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Implementation of Finite Mixture Model\n",
    "\n",
    "<details>\n",
    "  <summary>Table of Contents</summary>\n",
    "  <ol>\n",
    "    <li>\n",
    "      Import Needed Filepaths and Libraries\n",
    "    </li>\n",
    "    <li>\n",
    "    Load Dataset Into Pandas DataFrame\n",
    "    </li>\n",
    "    <li>\n",
    "    Define Response Variable `y`\n",
    "    </li>\n",
    "    <li>\n",
    "    Set Up Reproducible Random Number Generator\n",
    "    </li>\n",
    "    <li>\n",
    "      Setting Up Mixture Model\n",
    "      <ul>\n",
    "        <li>Initial Parameters</li>\n",
    "        <li>Setting Priors</li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    <li>Gibbs Sampler Implementation\n",
    "      <ul>\n",
    "        <li>Gibbs Sampler Updates</li>\n",
    "      </ul>\n",
    "</li>\n",
    "  </ol>\n",
    "</details>"
   ],
   "id": "ccd66e60768a4185"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The beauty of Bayesian techniques over other data science techniques is that many data points are not required. Our objective is to model the distribution of .\n",
    "\n",
    "$y_{i}$. $z_{i}$ is our indicator variable that indicates what type. If it's zero, then, and if it's one, then. Lambda is the mixing portion. It's what percent of are versus."
   ],
   "id": "185ba8b95ea18e7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Needed Filepaths and Libraries",
   "id": "f255d2bca1da6a32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from texas_gerrymandering_hb4.config import FINAL_CSV"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Dataset Into Pandas DataFrame\n",
    "Our processed dataset is read into a Pandas DataFrame."
   ],
   "id": "1a541f02f49af950"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.read_csv(FINAL_CSV)",
   "id": "ccbaa98efb2b2226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare Outcomes",
   "id": "e7f0df3b9b022143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Defining Response Variable `y`\n",
    "* `y` represents the Democratic vote share per district."
   ],
   "id": "f7bfdf303a2da38b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y = df[\"dem_share\"].values.astype(float)",
   "id": "6b77e33eb6958db1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Defining Sample Size\n",
    "* `n` denotes the sample size, which is the number of Congressional districts in Texas (38)."
   ],
   "id": "8dfe4e5d79c11cec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "n = len(y)",
   "id": "241868c9c4076c41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding Racial Minority Composition Column to DataFrame\n",
    "* These values will help interpret the mixture components in terms of race."
   ],
   "id": "72c00e632ff19477"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[\"pct_minority\"] = 1 - df[\"pct_white\"]",
   "id": "93140c155702d2ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting Up Mixture Model",
   "id": "2e8a84b6874592cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set Up Reproducible Random Number Generator",
   "id": "b10c63bc76aa36aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.random.seed(123)",
   "id": "9c8b9b6581dd21e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initial Parameters\n",
    "* These parameters represent starting guesses for the Gibbs sampler.\n",
    "* `sigma_squared_1` and `sigma_squared_2` are the initial variances for each component."
   ],
   "id": "e0b89c9d34b110cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Initialize $\\lambda$\n",
    "`lambda` ($\\lambda$) is the mixing proportion. Starting from a place of ignorance, we assume a perfect mix between the two, and that our means are the sample means and our variances are the sample variances. $\\lambda$ is set to 0.5 to serve as a neutral initial guess."
   ],
   "id": "f5ea9c80a3452bae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "_lambda = 0.5",
   "id": "516df955d0575b62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Initialize Means: $\\mu_1$ and $\\mu_2$\n",
    "* `mu_1` and `mu_2` represent the initial means for each component."
   ],
   "id": "400ac6114d260764"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mu_1 = np.mean(y)\n",
    "mu_2 = np.mean(y)"
   ],
   "id": "a391300f3ce87501"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Initialize Variances: $\\sigma^2_1$ and $\\sigma^2_2$\n",
    "* `sigma_squared_1` and `sigma_squared_2` are the initial variances for each component."
   ],
   "id": "fbd80058db7b57e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sigma_squared_1 = np.var(y,ddof=1)\n",
    "sigma_squared_2 = np.var(y, ddof=1)"
   ],
   "id": "bcf102058705b05b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setting Priors\n",
    "* Our variances are coming from the scaled inverse-$\\chi^2$ distribution with 2 degrees of freedom."
   ],
   "id": "4e5022eff854c381"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Setting Priors for $\\lambda$: $\\alpha_1$ and $\\alpha_2$\n",
    "* `alpha_1` and `alpha_2` are the priors for lambda.\n",
    "$$\\lambda \\sim Beta(\\alpha_2, \\alpha_2)$$\n",
    "Because the prior for lambda is Beta(2,2), `alpha_1` and `alpha_2` are both set to 2.\n",
    "* This is a conjugate prior. Also, as a reminder, Beta(2,2) is also a Dirchilet distribution. What is beneficial about this is that the probability of obtaining 0 or 1, which is a degenerate model, is 0. Hence, as you get closer to 0 or 1, the likelihood is tiny. As a result, the problem is pushed further away from a degenerate value, so they become less likely to accidently become a point of convergence."
   ],
   "id": "4a5d12cb2e8fc135"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "alpha_1 = 2\n",
    "alpha_2 = 2"
   ],
   "id": "9ef6d57f719e0b0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Setting Priors for Means: $\\mu_1$ and $\\mu_2$\n",
    "* The means `m0_1` and `mu0_2` are coming from the same distribution."
   ],
   "id": "df243f3d4c724da2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mu0_1 = np.mean(y)\n",
    "mu0_2 = np.mean(y)"
   ],
   "id": "19927a1b1f7789ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setting Priors for Variances: $\\sigma_1^2$ and $\\sigma_2^2$",
   "id": "6ae9f9ffbbc28fc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sigma_0_squared_1 = 0.20**2\n",
    "sigma_0_squared_2 = 0.20**2"
   ],
   "id": "ccaffe6b07362712"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setting Priors for Degrees of Freedom",
   "id": "2d58c877e10b415f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "degrees_of_freedom0_1 = 2\n",
    "degrees_of_freedom0_2 = 2"
   ],
   "id": "21a45c5b749ff24a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gibbs Sampler Parameters",
   "id": "5636e5c6997b8d8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "iterations = 1000\n",
    "warmup = 500"
   ],
   "id": "d05c694656d30ce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Storage for Samples",
   "id": "ad4034fa28adb96f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lambda_samples = np.zeros(iterations)\n",
    "mu1_samples = np.zeros(iterations)\n",
    "mu2_samples = np.zeros(iterations)\n",
    "sigma_squared_1_samples = np.zeros(iterations)\n",
    "sigma_squared_2_samples = np.zeros(iterations)\n",
    "z_samples = np.zeros((iterations, n), dtype=int)"
   ],
   "id": "154c323de0cd0651"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Scaled Inverse-$\\chi^2$ Sampler\n",
    "* This is the conjugate prior for the variance in the normal model.\n",
    "* This will be used to update $\\sigma_1^2$ and $\\sigma_2^2$."
   ],
   "id": "1eff9fd61609a33e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rinvchisq(df, scale):\n",
    "    return df * scale / np.random.chisquare(df)\n"
   ],
   "id": "bf8ce7048c5d348e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gibbs Sampler Implementation\n",
    "* The process for the Gibbs sampler involves initializing parameters, iterative sampling, and continuing the iterations until convergence.\n",
    "* The benefits of the Gibbs sampler are that it handles complex, high-dimensional distributions and is easier to implement than direct sampling methods."
   ],
   "id": "f9a98a9f45cc5a0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  Gibbs Sampler Updates",
   "id": "1fecc588b3e69dfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Updating z\n",
    "* We compute the posterior probability that our $z_{i}=1$, taking into account the normal distributions that our data follows:\n",
    "$$p_{z_{i}=1} = \\frac{\\lambda_{old} \\cdot N(y_{i}|\\mu_{1,init}, \\sigma^2_{1,init})}{\\lambda_{old} \\cdot N(y_{i}|\\mu_{1,init}, \\sigma_{1,init}^2) + (1-\\lambda_{old})N(y_{i}|\\mu_{1,init}, \\sigma_{1,init}^2)}$$\n",
    "* Once the probability of $z_{i=1}$ is computed, then a new value of $z_{i}$ is computed using a binomial distribution.\n",
    "$$z_{i}^{new} \\leftarrow Bin(n, p_{z_{i}=1})$$"
   ],
   "id": "bc65f83e839b2925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Updating $\\lambda$\n",
    "* Updating $\\lambda$ is a two-step process because it follows a Beta distribution.\n",
    "* First, we update the Beta distribution using the standard form from conjugacy. Recall that our parameters are (2,2); they are being updated based on the frequency of 1's among our z's, which is why we end up with two updates for $\\alpha$ and $\\beta$. We begin updating $\\lambda$ by first updating $\\alpha$ and $\\beta$ parameters:\n",
    "$$\\alpha_{new} \\leftarrow \\alpha_{old} + \\sum z_{i}^{new}$$\n",
    "$$\\beta_{new} \\leftarrow \\beta_{old} + n -  \\sum z_{i}^{new}$$\n",
    "* We update $\\lambda$ by computing the posterior probability. Because our $\\lambda$ is coming from a Beta distribution with updated parameters, we draw a value of $\\lambda$:\n",
    "$$\\lambda_{new} \\leftarrow Beta(\\alpha_{new}, \\beta_{new})$$"
   ],
   "id": "128842edeee97352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Updating Means: $\\mu_1$ and $\\mu_2$\n",
    "* Our number of observations are $n_1 = \\sum z_{i}^{new}$ and $n_2 = \\sum z_{i}^{new}$, respectively. Recall that our $z_{i}$ values have been redrawn.\n",
    "* We examine the total number of observations, followed by the means.  We compute the means as follows:\n",
    "$$\\bar{y}_1 = \\frac{1}{n_1} \\sum y_{i} (z_{i}^{new} = 1)$$\n",
    "$$\\bar{y}_2 = \\frac{1}{n_2} \\sum y_{i} (z_{i}^{new} = 1)$$\n",
    "* Once we obtain our means, we are able to compute a posterior mean based our sample. We construct this using our initial values; we use `init` to denote this. This is combined with our prior and sample to compute a new mean as follows:\n",
    "$$\\mu_{1,new} = \\frac{\\frac{\\mu}{\\sigma_{1,init}^2} + n_1 \\frac{\\bar{y}_1}{\\sigma^2_{1, old}}}{\\frac{1}{\\sigma_{1,init}^2}}$$"
   ],
   "id": "e4d331c52d740338"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Updating Variances: $\\sigma_1^2$ and $\\sigma_2^2$\n",
    "* We compute the posterior distribution parameters for $\\chi^2$(\\alpha^2):\n",
    "$$\\alpha_{new}^2 \\leftarrow \\frac{\\alpha_1^2 + n_1 s^2}{}$$"
   ],
   "id": "cb50ece2e859041c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(iterations):\n",
    "    # ---- Update z (component membership for each district) ----\n",
    "    lik_1 = _lambda * norm.pdf(y, loc=mu_1, scale=np.sqrt(sigma_squared_1))\n",
    "    lik_2 = (1 - _lambda) * norm.pdf(y, loc=mu_2, scale=np.sqrt(sigma_squared_2))\n",
    "    z_probs = lik_1 / (lik_1 + lik_2)\n",
    "    z = np.random.binomial(1, z_probs)  # 1 = component 1, 0 = component 2\n",
    "\n",
    "    # ---- Update lambda ----\n",
    "    alpha_lambda_post = alpha_1 + np.sum(z)\n",
    "    beta_lambda_post = alpha_2 + (n - np.sum(z))\n",
    "    _lambda = np.random.beta(alpha_lambda_post, beta_lambda_post)\n",
    "\n",
    "    # ---- Update mu_1 ----\n",
    "    n1 = np.sum(z)\n",
    "    if n1 > 0:\n",
    "        y1_mean = np.mean(y[z == 1])\n",
    "        mu1_post_mean = ((mu0_1 / sigma_0_squared_1) + n1 * y1_mean / sigma_squared_1) / \\\n",
    "                        (1 / sigma_0_squared_1 + n1 / sigma_squared_2)\n",
    "        mu1_post_sd = np.sqrt(1 / (1 / sigma_0_squared_1 + n1 / sigma_squared_1))\n",
    "    else:\n",
    "        mu1_post_mean = mu0_1\n",
    "        mu1_post_sd = np.sqrt(sigma_0_squared_1)\n",
    "    mu_1 = np.random.normal(mu1_post_mean, mu1_post_sd)\n",
    "\n",
    "    # ---- Update mu_2 ----\n",
    "    n2 = n - n1\n",
    "    if n2 > 0:\n",
    "        y2_mean = np.mean(y[z == 0])\n",
    "        mu2_post_mean = ((mu0_2 / sigma_0_squared_2) + n2 * y2_mean / sigma_squared_2) / \\\n",
    "                        (1 / sigma_0_squared_2 + n2 / sigma_squared_2)\n",
    "        mu2_post_sd = np.sqrt(1 / (1 / sigma_0_squared_2 + n2 / sigma_squared_2))\n",
    "    else:\n",
    "        mu2_post_mean = mu0_2\n",
    "        mu2_post_sd = np.sqrt(sigma_0_squared_2)\n",
    "    mu_2 = np.random.normal(mu2_post_mean, mu2_post_sd)\n",
    "\n",
    "    # ---- Update sigma_1_sq (scaled inverse-chi-squared) ----\n",
    "    nu1_post = degrees_of_freedom0_1 + n1\n",
    "    if n1 > 0:\n",
    "        ss1 = np.sum((y[z == 1] - mu_1) ** 2)\n",
    "    else:\n",
    "        ss1 = 0.0\n",
    "    sigma1_post_scale = (degrees_of_freedom0_1 * sigma_0_squared_1 + ss1) / nu1_post\n",
    "    sigma_1_sq = rinvchisq(nu1_post, sigma1_post_scale)\n",
    "\n",
    "    # ---- Update sigma_2_sq (scaled inverse-chi-squared) ----\n",
    "    nu2_post = degrees_of_freedom0_2 + n2\n",
    "    if n2 > 0:\n",
    "        ss2 = np.sum((y[z == 0] - mu_2) ** 2)\n",
    "    else:\n",
    "        ss2 = 0.0\n",
    "    sigma2_post_scale = (degrees_of_freedom0_2 * sigma_0_squared_2 + ss2) / nu2_post\n",
    "    sigma_2_sq = rinvchisq(nu2_post, sigma2_post_scale)\n",
    "\n",
    "    # ---- Store samples ----\n",
    "    lambda_samples[i] = _lambda\n",
    "    mu1_samples[i] = mu_1\n",
    "    mu2_samples[i] = mu_2\n",
    "    sigma_squared_1_samples[i] = sigma_1_sq\n",
    "    sigma_squared_2_samples[i] = sigma_2_sq\n",
    "    z_samples[i, :] = z"
   ],
   "id": "119836eb3c774020"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Posterior Summaries",
   "id": "e3f6c8f02b720c9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Discard Warmup Draws",
   "id": "92b38438971d8837"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lambda_post = lambda_samples[warmup:]\n",
    "mu1_post = mu1_samples[warmup:]\n",
    "mu2_post = mu2_samples[warmup:]\n",
    "sigma1_post = sigma_squared_1_samples[warmup:]\n",
    "sigma2_post = sigma_squared_2_samples[warmup:]\n",
    "z_post = z_samples[warmup:, :]"
   ],
   "id": "c4add9271daa9ca0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Calculate Means for Each Parameter",
   "id": "c0499a52154d8e67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mean_lambda = lambda_post.mean()\n",
    "mean_mu1 = mu1_post.mean()\n",
    "mean_mu2 = mu2_post.mean()"
   ],
   "id": "5a7f188839a5fe93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Print Posterior Means",
   "id": "28966c1b5b4cc590"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Posterior mean lambda:\", mean_lambda)\n",
    "print(\"Posterior mean mu_1 (component 1):\", mean_mu1)\n",
    "print(\"Posterior mean mu_2 (component 2):\", mean_mu2)"
   ],
   "id": "e0539043b69dcd9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Determining Which Components Have Low Democratic Vote Share",
   "id": "612449d238b5b972"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if mean_mu1 < mean_mu2:\n",
    "    gerr_comp = 1\n",
    "else:\n",
    "    gerr_comp = 0"
   ],
   "id": "7f80d33ac230b6ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sort Districts by Probability of Being Gerrymandered",
   "id": "b6bff3b1c75a2921"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_sorted = df.sort_values(\"post_prob_gerrymandered\", ascending=False)\n",
    "\n",
    "print(\"\\nTop districts by posterior probability of being in the low-dem-share component:\")\n",
    "print(df_sorted[[\n",
    "    \"district_id\",\n",
    "    \"dem_share\",\n",
    "    \"rep_share\",\n",
    "    \"pct_white\",\n",
    "    \"pct_minority\",\n",
    "    \"polsby_popper\",\n",
    "    \"reock\",\n",
    "    \"post_prob_gerrymandered\"\n",
    "]].head(10))"
   ],
   "id": "45216ae99e00fb12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scatterplot",
   "id": "1f8ba4fbc374f754"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Scatter: pct_minority vs dem_share, colored by prob gerrymandered\n",
    "plt.figure(figsize=(7, 5))\n",
    "sc = plt.scatter(\n",
    "    df[\"pct_minority\"],\n",
    "    df[\"dem_share\"],\n",
    "    c=df[\"post_prob_gerrymandered\"],\n",
    "    cmap=\"viridis\"\n",
    ")\n",
    "plt.colorbar(sc, label=\"Posterior P(gerrymandered component)\")\n",
    "plt.xlabel(\"Percent Minority (1 - pct_white)\")\n",
    "plt.ylabel(\"Democratic Vote Share\")\n",
    "plt.title(\"Pct Minority vs Dem Share\\nColor = P(low-dem-share component)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "dd7faa61c59df0b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Histogram of Components by Democratic Vote Share",
   "id": "41be917207702142"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "map_assign = (df[\"post_prob_gerrymandered\"] > 0.5).astype(int)\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.hist(y[map_assign == 0], bins=10, alpha=0.7, label=\"Not gerrymandered (MAP)\")\n",
    "plt.hist(y[map_assign == 1], bins=10, alpha=0.7, label=\"Gerrymandered (MAP)\")\n",
    "plt.xlabel(\"Democratic Vote Share\")\n",
    "plt.ylabel(\"Count of districts\")\n",
    "plt.title(\"Mixture Components over Dem Vote Share\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "e14799ff5d6eb162"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myvenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
