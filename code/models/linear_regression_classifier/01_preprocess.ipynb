{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T22:58:52.731009Z",
     "start_time": "2025-09-30T22:58:52.726548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add near the top (after imports)\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "class SafeJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        # pathlib.Path -> str\n",
    "        if isinstance(o, pathlib.Path):\n",
    "            return str(o)\n",
    "        # NumPy scalars -> native Python\n",
    "        if isinstance(o, (np.integer, np.floating, np.bool_)):\n",
    "            return o.item()\n",
    "        # NumPy arrays -> lists\n",
    "        if isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "        return super().default(o)\n"
   ],
   "id": "bd54c6e4b967a2d5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T22:58:52.813113Z",
     "start_time": "2025-09-30T22:58:52.779019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Imports ---\n",
    "import json, os, re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "from texas_gerrymandering_hb4.config import FINAL_CSV\n",
    "\n",
    "# --- Paths ---\n",
    "ARTIFACT_DIR = \"artifacts\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "XTR_PATH = os.path.join(ARTIFACT_DIR, \"X_train.npz\")\n",
    "XTE_PATH = os.path.join(ARTIFACT_DIR, \"X_test.npz\")\n",
    "YTR_PATH = os.path.join(ARTIFACT_DIR, \"y_train.csv\")\n",
    "YTE_PATH = os.path.join(ARTIFACT_DIR, \"y_test.csv\")\n",
    "TRANSFORMER_PATH = os.path.join(ARTIFACT_DIR, \"preprocessor.joblib\")\n",
    "META_PATH = os.path.join(ARTIFACT_DIR, \"meta.json\")\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv(FINAL_CSV)\n",
    "print(\"Loaded:\", df.shape, \"columns:\", list(df.columns))\n",
    "\n",
    "# --- Helper: drop obvious non-features if present ---\n",
    "drop_cols = []\n",
    "for c in [\"district_id\", \"district\", \"DISTRICT\", \"District\", \"geometry\", \"geom\", \"wkt\"]:\n",
    "    if c in df.columns:\n",
    "        drop_cols.append(c)\n",
    "\n",
    "# --- Auto-detect/construct the binary target ---\n",
    "# Preferred categorical targets (exact or close match)\n",
    "cat_target_candidates = [\n",
    "    \"party\", \"winner\", \"party_outcome\", \"party_winner\", \"outcome\"\n",
    "]\n",
    "cat_target = next((c for c in cat_target_candidates if c in df.columns), None)\n",
    "\n",
    "def pick_positive_label(labels):\n",
    "    \"\"\"Prefer a GOP-ish label if present; else pick the lexicographically last for stability.\"\"\"\n",
    "    labels_norm = [str(l).lower() for l in labels]\n",
    "    gop_like = [\"rep\", \"republican\", \"gop\", \"r\"]\n",
    "    for gl in gop_like:\n",
    "        for lab, labn in zip(labels, labels_norm):\n",
    "            if re.search(rf\"\\b{gl}\\b\", labn):\n",
    "                return lab\n",
    "    # fallback: if \"Democrat\" appears, use that; else lexicographic max\n",
    "    for lab, labn in zip(labels, labels_norm):\n",
    "        if re.search(r\"\\bdem|democrat|democratic\\b\", labn):\n",
    "            return lab\n",
    "    return sorted(map(str, labels))[-1]\n",
    "\n",
    "y_series = None\n",
    "origin_target_col = None\n",
    "positive_label = None\n",
    "derived_from = None\n",
    "\n",
    "if cat_target is not None and df[cat_target].notna().any():\n",
    "    origin_target_col = cat_target\n",
    "    y_raw = df[cat_target].astype(str)\n",
    "    labels = sorted(y_raw.dropna().unique().tolist())\n",
    "    if len(labels) < 2:\n",
    "        raise ValueError(f\"Categorical target '{cat_target}' has <2 unique values: {labels}\")\n",
    "    # Reduce to binary if more than 2 classes by mapping the most common two\n",
    "    top2 = y_raw.value_counts().index.tolist()[:2]\n",
    "    y_raw = y_raw.where(y_raw.isin(top2))\n",
    "    y_raw = y_raw.dropna()\n",
    "    df = df.loc[y_raw.index].copy()\n",
    "    positive_label = pick_positive_label(top2)\n",
    "    y_series = (y_raw == positive_label).astype(int)\n",
    "    derived_from = f\"categorical:{cat_target}:{top2}\"\n",
    "else:\n",
    "    # Try numeric share columns\n",
    "    share_cols_priority = [\n",
    "        (\"rep_share\",  \"Republican\"),\n",
    "        (\"gop_share\",  \"Republican\"),\n",
    "        (\"republican_share\", \"Republican\"),\n",
    "        (\"r_share\",    \"Republican\"),\n",
    "        (\"dem_share\",  \"Democrat\"),\n",
    "        (\"democratic_share\", \"Democrat\"),\n",
    "        (\"d_share\",    \"Democrat\"),\n",
    "    ]\n",
    "    found = None\n",
    "    for col, pos_lab in share_cols_priority:\n",
    "        if col in df.columns:\n",
    "            found = (col, pos_lab); break\n",
    "    if not found:\n",
    "        raise ValueError(\n",
    "            \"Could not find a target. Provide a categorical party column \"\n",
    "            \"(e.g., 'party'/'winner') or a share column (e.g., 'rep_share' or 'dem_share').\"\n",
    "        )\n",
    "    col, pos_lab = found\n",
    "    origin_target_col = col\n",
    "    # Build binary: pos_lab (Republican or Democrat) means 1 if that share >= 0.5\n",
    "    if pos_lab == \"Republican\":\n",
    "        y_series = (df[col] >= 0.5).astype(int)\n",
    "        positive_label = \"Republican\"\n",
    "    else:\n",
    "        # pos_lab == \"Democrat\": 1 means Democrat wins; we still want a single \"positive_label\"\n",
    "        y_series = (df[col] >= 0.5).astype(int)\n",
    "        positive_label = \"Democrat\"\n",
    "    derived_from = f\"numeric_share:{col}\"\n",
    "\n",
    "# Align feature matrix with y index (in case we filtered)\n",
    "X = df.drop(columns=[c for c in [origin_target_col] if c in df.columns] + drop_cols)\n",
    "y = y_series\n",
    "\n",
    "# --- Identify feature types ---\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "\n",
    "print(\"Dropping non-features:\", drop_cols)\n",
    "print(\"Categorical features:\", categorical_cols)\n",
    "print(\"Numeric features:\", numeric_cols)\n",
    "print(\"Target positive label:\", positive_label, \"| derived_from:\", derived_from)\n",
    "\n",
    "# --- Split (force both classes in test if possible) ---\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "y_arr = np.asarray(y, dtype=int)\n",
    "unique, counts = np.unique(y_arr, return_counts=True)\n",
    "class_counts = dict(zip(unique.tolist(), counts.tolist()))\n",
    "print(\"Class counts (all data):\", class_counts)\n",
    "\n",
    "# Aim for ~ 25â€“33% test, but ensure at least 1 of each class in test if both classes exist\n",
    "test_size = 0.33 if len(unique) == 2 and min(counts) >= 3 else 0.25\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=100, test_size=test_size, random_state=42)\n",
    "X_train = X_test = y_train = y_test = None\n",
    "\n",
    "for tr_idx, te_idx in sss.split(X, y_arr):\n",
    "    # ensure both classes appear in test\n",
    "    if len(np.unique(y_arr[te_idx])) == len(unique):\n",
    "        X_train, X_test = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_train, y_test = y_arr[tr_idx], y_arr[te_idx]\n",
    "        break\n",
    "\n",
    "# Fallback: if we still couldn't get both classes (e.g., extremely imbalanced tiny data)\n",
    "if X_train is None:\n",
    "    print(\"WARNING: Could not form a test split containing both classes. Proceeding with best stratified split.\")\n",
    "    tr_idx, te_idx = next(iter(StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=43).split(X, y_arr)))\n",
    "    X_train, X_test = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "    y_train, y_test = y_arr[tr_idx], y_arr[te_idx]\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0], \"Test size:\", X_test.shape[0])\n",
    "print(\"Test class counts:\", dict(zip(*np.unique(y_test, return_counts=True))))\n",
    "\n",
    "\n",
    "# --- Preprocessor (fit ONLY on train) ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# --- Transform ---\n",
    "X_train_t = preprocessor.transform(X_train)\n",
    "X_test_t  = preprocessor.transform(X_test)\n",
    "\n",
    "# --- Persist ---\n",
    "sparse.save_npz(XTR_PATH, sparse.csr_matrix(X_train_t))\n",
    "sparse.save_npz(XTE_PATH, sparse.csr_matrix(X_test_t))\n",
    "pd.Series(y_train).to_csv(YTR_PATH, index=False, header=False)\n",
    "pd.Series(y_test).to_csv(YTE_PATH, index=False, header=False)\n",
    "joblib.dump(preprocessor, TRANSFORMER_PATH)\n",
    "\n",
    "# Feature names (best effort)\n",
    "try:\n",
    "    cat_names = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols).tolist() if categorical_cols else []\n",
    "    feat_names = cat_names + numeric_cols\n",
    "except Exception:\n",
    "    feat_names = [f\"f{i}\" for i in range(X_train_t.shape[1])]\n",
    "\n",
    "meta = {\n",
    "    \"source_csv\": str(FINAL_CSV),                     # ensure string\n",
    "    \"origin_target_col\": origin_target_col,\n",
    "    \"derived_from\": derived_from,\n",
    "    \"positive_label\": positive_label,\n",
    "    \"dropped_non_features\": drop_cols,\n",
    "    \"categorical_cols\": list(map(str, categorical_cols)),\n",
    "    \"numeric_cols\": list(map(str, numeric_cols)),\n",
    "    \"n_train_raw\": int(X_train.shape[0]),\n",
    "    \"n_test_raw\": int(X_test.shape[0]),\n",
    "    \"n_features_transformed\": int(X_train_t.shape[1]),\n",
    "    \"feature_names\": list(map(str, feat_names)),\n",
    "}\n",
    "with open(META_PATH, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2, cls=SafeJSONEncoder)  # <-- use custom encoder\n",
    "\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(f\"Train (X) transformed: {X_train_t.shape} | Test (X): {X_test_t.shape}\")\n"
   ],
   "id": "152588a8201bd14f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (38, 11) columns: ['district_id', 'polsby_popper', 'schwartzberg', 'convex_hull_ratio', 'reock', 'pct_white', 'pct_black', 'pct_asian', 'pct_hispanic', 'dem_share', 'rep_share']\n",
      "Dropping non-features: ['district_id']\n",
      "Categorical features: []\n",
      "Numeric features: ['polsby_popper', 'schwartzberg', 'convex_hull_ratio', 'reock', 'pct_white', 'pct_black', 'pct_asian', 'pct_hispanic', 'dem_share']\n",
      "Target positive label: Republican | derived_from: numeric_share:rep_share\n",
      "Class counts (all data): {1: 38}\n",
      "Train size: 28 Test size: 10\n",
      "Test class counts: {np.int64(1): np.int64(10)}\n",
      "Preprocessing complete.\n",
      "Train (X) transformed: (28, 9) | Test (X): (10, 9)\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
