{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 1: Imports & paths ---\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# diagnostics\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "IN = Path(\"../data/interim\")\n",
    "OUT = Path(\"../models\")\n",
    "DIA = OUT / \"diagnostics\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "DIA.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "2e79a7b0787998a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 2: Load data & meta ---\n",
    "X_train = pd.read_parquet(IN / \"X_train.parquet\")\n",
    "y_train = pd.read_parquet(IN / \"y_train.parquet\")[\"party\"]\n",
    "\n",
    "with open(IN / \"split_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "NUMERIC = meta[\"numeric\"]\n",
    "CATEGORICAL = meta[\"categorical\"]"
   ],
   "id": "6ded369bab668d40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 3: Build base pipeline ---\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(drop=\"first\", sparse_output=False), CATEGORICAL),\n",
    "    (\"num\", StandardScaler(), NUMERIC),\n",
    "])\n",
    "\n",
    "base_pipeline = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"reg\", LinearRegression())\n",
    "])"
   ],
   "id": "4c54e998bc629ac9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 4: Fit baseline & threshold tuning ---\n",
    "base_pipeline.fit(X_train, y_train)\n",
    "y_score_train_base = base_pipeline.predict(X_train).clip(0, 1)\n",
    "\n",
    "def pick_threshold(y_true, scores, metric=\"balanced_accuracy\"):\n",
    "    scores = np.asarray(scores)\n",
    "    grid = np.linspace(0.0, 1.0, 201)\n",
    "    best_thr, best_val = 0.5, -1.0\n",
    "    for thr in grid:\n",
    "        y_hat = (scores >= thr).astype(int)\n",
    "        if metric == \"balanced_accuracy\":\n",
    "            val = balanced_accuracy_score(y_true, y_hat)\n",
    "        else:\n",
    "            from sklearn.metrics import f1_score\n",
    "            val = f1_score(y_true, y_hat, zero_division=0)\n",
    "        if val > best_val:\n",
    "            best_val, best_thr = val, thr\n",
    "    return float(best_thr), float(best_val)\n",
    "\n",
    "thr_base, bal_train_base = pick_threshold(y_train, y_score_train_base, \"balanced_accuracy\")\n",
    "print(f\"[BASE] threshold={thr_base:.3f}, balanced-acc(train)={bal_train_base:.3f}\")"
   ],
   "id": "b42e933061dbebde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 5: Recover train design matrix & feature names for diagnostics ---\n",
    "# Use the fitted preprocessor to get dense design matrix\n",
    "X_train_design = base_pipeline.named_steps[\"pre\"].transform(X_train)  # numpy array\n",
    "# Compose names: OHE then numeric\n",
    "ohe = base_pipeline.named_steps[\"pre\"].named_transformers_[\"cat\"]\n",
    "ohe_names = list(ohe.get_feature_names_out(CATEGORICAL))\n",
    "feat_names = ohe_names + NUMERIC\n",
    "\n",
    "# Add intercept for statsmodels\n",
    "X_sm = sm.add_constant(X_train_design)\n",
    "model_sm = sm.OLS(y_train.values, X_sm).fit()"
   ],
   "id": "35bf8bf6a441a298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 6: VIF diagnostics ---\n",
    "vif_data = []\n",
    "for i in range(1, X_sm.shape[1]):  # skip intercept at 0\n",
    "    vif_val = variance_inflation_factor(X_sm[:, 1:], i-1)\n",
    "    vif_data.append((feat_names[i-1], float(vif_val)))\n",
    "\n",
    "vif_df = pd.DataFrame(vif_data, columns=[\"feature\", \"VIF\"]).sort_values(\"VIF\", ascending=False)\n",
    "vif_df.to_csv(DIA / \"vif_baseline.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(vif_df[\"feature\"], vif_df[\"VIF\"])\n",
    "plt.title(\"VIF (Baseline, Train)\")\n",
    "plt.xlabel(\"VIF\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIA / \"vif_baseline.png\", dpi=200)\n",
    "plt.close()"
   ],
   "id": "6c88719e9801248"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 7: Residual diagnostics (baseline) ---\n",
    "residuals = y_train.values - y_score_train_base\n",
    "fitted = y_score_train_base\n",
    "\n",
    "# residuals vs fitted\n",
    "plt.figure()\n",
    "plt.scatter(fitted, residuals)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.title(\"Residuals vs Fitted (Baseline, Train)\")\n",
    "plt.xlabel(\"Fitted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIA / \"residuals_vs_fitted_baseline.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# histogram of residuals\n",
    "plt.figure()\n",
    "plt.hist(residuals, bins=12)\n",
    "plt.title(\"Residuals Histogram (Baseline, Train)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIA / \"residuals_hist_baseline.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# QQ plot\n",
    "fig = sm.qqplot(residuals, line='45', fit=True)\n",
    "plt.title(\"QQ Plot of Residuals (Baseline, Train)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(DIA / \"qqplot_residuals_baseline.png\", dpi=200)\n",
    "plt.close()"
   ],
   "id": "755d0ed9f2a7a75c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 8: Influence/Cook's distance (baseline) ---\n",
    "influence = model_sm.get_influence()\n",
    "cooks_d = influence.cooks_distance[0]  # array\n",
    "leverage = influence.hat_matrix_diag\n",
    "\n",
    "infl_df = pd.DataFrame({\n",
    "    \"index\": X_train.index,\n",
    "    \"cooks_d\": cooks_d,\n",
    "    \"leverage\": leverage,\n",
    "    \"residual\": residuals\n",
    "}).set_index(\"index\").sort_values(\"cooks_d\", ascending=False)\n",
    "infl_df.to_csv(DIA / \"influence_baseline.csv\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(leverage, residuals)\n",
    "plt.title(\"Leverage vs Residuals (Baseline, Train)\")\n",
    "plt.xlabel(\"Leverage\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(DIA / \"leverage_vs_residuals_baseline.png\", dpi=200)\n",
    "plt.close()"
   ],
   "id": "840376c1f05e39c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 9: Remedial options (auto) ---\n",
    "# 9a. High multicollinearity: drop features with VIF > 10 and refit\n",
    "VIF_THRESHOLD = 10.0\n",
    "high_vif_features = vif_df.loc[vif_df[\"VIF\"] > VIF_THRESHOLD, \"feature\"].tolist()\n",
    "\n",
    "def drop_features_in_X(X: pd.DataFrame, to_drop_ohe_names: list):\n",
    "    \"\"\"\n",
    "    For OHE features, each name corresponds to a one-hot column, not original input.\n",
    "    We can drop high-VIF OHE features by setting OneHotEncoder to drop specific categories.\n",
    "    For simplicity, we *filter columns after transformation* using a custom selector.\n",
    "    Here, we rebuild a new ColumnTransformer that excludes those specific OHE columns.\n",
    "    \"\"\"\n",
    "    # Build a mask of OHE columns to keep\n",
    "    ohe_keep = [name for name in ohe_names if name not in to_drop_ohe_names]\n",
    "    return ohe_keep\n",
    "\n",
    "models = {\n",
    "    \"baseline\": {\"pipeline\": base_pipeline, \"thr\": thr_base, \"bal_train\": bal_train_base}\n",
    "}\n",
    "\n",
    "if high_vif_features:\n",
    "    # Build a filterable pipeline by selecting OHE columns post-transform\n",
    "    # Approach: use preprocessor as-is but append a SelectFromColumns step (custom)\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "    class ColumnFilter(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, keep_ohe_names, numeric_names):\n",
    "            self.keep_ohe_names = keep_ohe_names\n",
    "            self.numeric_names = numeric_names\n",
    "            self.feature_names_ = keep_ohe_names + numeric_names\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            # X is the dense array [OHE || scaled numeric]\n",
    "            ohe_count = len(ohe_names)\n",
    "            # Indices to keep among OHE\n",
    "            indices_ohe = [ohe_names.index(n) for n in self.keep_ohe_names]\n",
    "            X_ohe = X[:, indices_ohe]\n",
    "            X_num = X[:, len(ohe_names):]\n",
    "            return np.concatenate([X_ohe, X_num], axis=1)\n",
    "\n",
    "    keep_ohe = drop_features_in_X(X_train, high_vif_features)\n",
    "    col_filter = ColumnFilter(keep_ohe_names=keep_ohe, numeric_names=NUMERIC)\n",
    "\n",
    "    vif_pruned_pipeline = Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"filter\", col_filter),\n",
    "        (\"reg\", LinearRegression())\n",
    "    ])\n",
    "    vif_pruned_pipeline.fit(X_train, y_train)\n",
    "    y_score_train_vif = vif_pruned_pipeline.predict(X_train).clip(0, 1)\n",
    "    thr_vif, bal_train_vif = pick_threshold(y_train, y_score_train_vif, \"balanced_accuracy\")\n",
    "    print(f\"[VIF-PRUNED] drop={high_vif_features}, thr={thr_vif:.3f}, bal-acc(train)={bal_train_vif:.3f}\")\n",
    "\n",
    "    models[\"vif_pruned\"] = {\n",
    "        \"pipeline\": vif_pruned_pipeline,\n",
    "        \"thr\": thr_vif,\n",
    "        \"bal_train\": bal_train_vif,\n",
    "        \"metadata\": {\"dropped_ohe_features\": high_vif_features}\n",
    "    }\n",
    "\n",
    "# 9b. Influence pruning: remove points with Cook's D > 4/n and refit\n",
    "n = len(X_train)\n",
    "COOKS_THRESHOLD = 4 / n\n",
    "keep_mask = cooks_d < COOKS_THRESHOLD\n",
    "\n",
    "if keep_mask.mean() < 0.95:\n",
    "    # Only apply if more than 5% flagged (to avoid overfitting to tiny changes)\n",
    "    X_keep = X_train.loc[keep_mask]\n",
    "    y_keep = y_train.loc[keep_mask]\n",
    "\n",
    "    infl_pruned_pipeline = Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (\"reg\", LinearRegression())\n",
    "    ])\n",
    "    infl_pruned_pipeline.fit(X_keep, y_keep)\n",
    "    y_score_train_infl = infl_pruned_pipeline.predict(X_train).clip(0, 1)  # evaluate on full train for comparison\n",
    "    thr_infl, bal_train_infl = pick_threshold(y_train, y_score_train_infl, \"balanced_accuracy\")\n",
    "    print(f\"[INFL-PRUNED] kept={keep_mask.sum()}/{n}, thr={thr_infl:.3f}, bal-acc(train)={bal_train_infl:.3f}\")\n",
    "\n",
    "    models[\"influence_pruned\"] = {\n",
    "        \"pipeline\": infl_pruned_pipeline,\n",
    "        \"thr\": thr_infl,\n",
    "        \"bal_train\": bal_train_infl,\n",
    "        \"metadata\": {\"cooks_threshold\": COOKS_THRESHOLD, \"kept\": int(keep_mask.sum()), \"n\": int(n)}\n",
    "    }"
   ],
   "id": "1c0a958baf9292c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 10: Pick best train model variant (by balanced accuracy on train) ---\n",
    "best_name = max(models.keys(), key=lambda k: models[k][\"bal_train\"])\n",
    "best = models[best_name]\n",
    "print(f\"[SELECT] Best variant: {best_name} (bal-acc(train)={best['bal_train']:.3f})\")"
   ],
   "id": "7c72cd59a0cca375"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Cell 11: Save active model, threshold, and artifacts ---\n",
    "joblib.dump(best[\"pipeline\"], OUT / \"active_model.pkl\")\n",
    "with open(OUT / \"train_threshold.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": best[\"thr\"], \"balanced_accuracy_on_train\": best[\"bal_train\"], \"variant\": best_name}, f, indent=2)\n",
    "\n",
    "# Also save baseline for reference\n",
    "joblib.dump(base_pipeline, OUT / \"linear_regression_pipeline_baseline.pkl\")\n",
    "\n",
    "# Save coefficient chart for active model\n",
    "# We must reconstruct the active feature names (could be filtered)\n",
    "pipe = best[\"pipeline\"]\n",
    "if \"filter\" in pipe.named_steps:\n",
    "    # Rebuild feature names using filter\n",
    "    keep_ohe = pipe.named_steps[\"filter\"].keep_ohe_names\n",
    "    feat_active = keep_ohe + NUMERIC\n",
    "    # Get coefficients from reg\n",
    "    coefs = pipe.named_steps[\"reg\"].coef_\n",
    "else:\n",
    "    feat_active = feat_names\n",
    "    coefs = pipe.named_steps[\"reg\"].coef_\n",
    "\n",
    "coef_df = pd.DataFrame({\"feature\": feat_active, \"coef\": coefs}).sort_values(\n",
    "    \"coef\", key=lambda s: s.abs(), ascending=False\n",
    ")\n",
    "coef_df.to_csv(OUT / \"linear_regression_coefficients_active.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(coef_df[\"feature\"], coef_df[\"coef\"])\n",
    "plt.title(f\"Linear Regression Coefficients ({best_name})\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT / \"linear_regression_coefficients_active.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# Save diagnostic summaries\n",
    "with open(OUT / \"training_summary.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"selected_variant\": best_name,\n",
    "        \"variants\": {k: {\"balanced_accuracy_train\": v[\"bal_train\"]} for k,v in models.items()},\n",
    "        \"high_vif_features\": high_vif_features if high_vif_features else [],\n",
    "        \"cooks_threshold\": COOKS_THRESHOLD,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Training complete. Saved active_model.pkl, threshold, coefficients, and diagnostics.\")\n"
   ],
   "id": "c5017289a7a568c8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
