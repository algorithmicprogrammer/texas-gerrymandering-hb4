{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL 0 — Imports and data loading\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import invgamma, multivariate_normal, norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7, 5)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"districts_final.csv\")\n",
    "\n",
    "# Keep only the columns we need\n",
    "cols = [\n",
    "    \"district_id\",\n",
    "    \"polsby_popper\",\n",
    "    \"schwartzberg\",\n",
    "    \"convex_hull_ratio\",\n",
    "    \"reock\",\n",
    "    \"pct_white\",\n",
    "    \"pct_black\",\n",
    "    \"pct_asian\",\n",
    "    \"pct_hispanic\",\n",
    "    \"dem_share\",\n",
    "    \"rep_share\",\n",
    "]\n",
    "df = df[cols].copy()\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 1 — Basic exploration\n",
    "\n",
    "df.describe()\n"
   ],
   "id": "52a313094dbfb8a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 2 — Visualize distributions\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 7))\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].hist(df[\"dem_share\"], bins=10)\n",
    "axes[0].set_title(\"Democratic share\")\n",
    "\n",
    "axes[1].hist(df[\"pct_black\"], bins=10)\n",
    "axes[1].set_title(\"Percent Black\")\n",
    "\n",
    "axes[2].hist(df[\"pct_hispanic\"], bins=10)\n",
    "axes[2].set_title(\"Percent Hispanic\")\n",
    "\n",
    "axes[3].hist(df[\"polsby_popper\"], bins=10)\n",
    "axes[3].set_title(\"Polsby-Popper\")\n",
    "\n",
    "axes[4].hist(df[\"reock\"], bins=10)\n",
    "axes[4].set_title(\"Reock\")\n",
    "\n",
    "axes[5].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "1d5f4b0532221599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 3 — Construct design matrix and standardize predictors\n",
    "\n",
    "# Outcome as proportion (ensure it's in [0,1])\n",
    "y = df[\"dem_share\"].values\n",
    "n = len(y)\n",
    "\n",
    "# Choose predictors\n",
    "predictors = [\n",
    "    \"pct_black\",\n",
    "    \"pct_hispanic\",\n",
    "    \"pct_asian\",\n",
    "    # leave out pct_white to avoid collinearity\n",
    "    \"polsby_popper\",\n",
    "    \"reock\",\n",
    "    \"schwartzberg\",\n",
    "]\n",
    "\n",
    "X_raw = df[predictors].values\n",
    "\n",
    "# Standardize predictors\n",
    "X_mean = X_raw.mean(axis=0)\n",
    "X_std = X_raw.std(axis=0, ddof=1)\n",
    "X_std[X_std == 0] = 1.0  # avoid divide-by-zero\n",
    "\n",
    "X = (X_raw - X_mean) / X_std\n",
    "\n",
    "# Add intercept column\n",
    "X = np.column_stack([np.ones(n), X])\n",
    "p = X.shape[1]\n",
    "\n",
    "predictors_with_intercept = [\"intercept\"] + predictors\n",
    "predictors_with_intercept\n"
   ],
   "id": "7df1aeec975f9846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 4 — Gibbs sampler for Bayesian linear regression\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Prior hyperparameters\n",
    "beta0 = np.zeros(p)\n",
    "tau2 = 10.0**2\n",
    "V0 = tau2 * np.eye(p)\n",
    "\n",
    "alpha0 = 2.0\n",
    "beta0_sigma = 1.0\n",
    "\n",
    "# Initialize\n",
    "beta_curr = np.zeros(p)\n",
    "sigma2_curr = 1.0\n",
    "\n",
    "n_iter = 20000\n",
    "burn_in = 5000\n",
    "\n",
    "beta_samples = np.zeros((n_iter, p))\n",
    "sigma2_samples = np.zeros(n_iter)\n",
    "\n",
    "XtX = X.T @ X\n",
    "Xty = X.T @ y\n",
    "V0_inv = np.linalg.inv(V0)\n",
    "\n",
    "for it in range(n_iter):\n",
    "    # --- Sample beta | sigma2, y ---\n",
    "    Vn_inv = XtX / sigma2_curr + V0_inv\n",
    "    Vn = np.linalg.inv(Vn_inv)\n",
    "    mn = Vn @ (Xty / sigma2_curr + V0_inv @ beta0)\n",
    "    beta_curr = multivariate_normal.rvs(mean=mn, cov=Vn)\n",
    "\n",
    "    # --- Sample sigma2 | beta, y ---\n",
    "    residual = y - X @ beta_curr\n",
    "    alpha_n = alpha0 + n / 2.0\n",
    "    beta_n = beta0_sigma + 0.5 * (residual @ residual)\n",
    "    sigma2_curr = invgamma.rvs(a=alpha_n, scale=beta_n)\n",
    "\n",
    "    beta_samples[it, :] = beta_curr\n",
    "    sigma2_samples[it] = sigma2_curr\n",
    "\n",
    "# Discard burn-in\n",
    "beta_post = beta_samples[burn_in:, :]\n",
    "sigma2_post = sigma2_samples[burn_in:]\n",
    "beta_post.shape, sigma2_post.shape\n"
   ],
   "id": "ad9047b25fdb4fc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 5 — Posterior summaries for beta\n",
    "\n",
    "def quantile_summary(samples, probs=(0.025, 0.25, 0.5, 0.75, 0.975)):\n",
    "    return np.quantile(samples, probs, axis=0)\n",
    "\n",
    "probs = (0.025, 0.25, 0.5, 0.75, 0.975)\n",
    "beta_q = quantile_summary(beta_post, probs=probs)\n",
    "\n",
    "summary_rows = []\n",
    "for j, name in enumerate(predictors_with_intercept):\n",
    "    row = {\n",
    "        \"parameter\": name,\n",
    "        \"mean\": beta_post[:, j].mean(),\n",
    "    }\n",
    "    for p_, qv in zip(probs, beta_q[:, j]):\n",
    "        row[f\"{int(p_ * 100)}%\"] = qv\n",
    "    summary_rows.append(row)\n",
    "\n",
    "beta_summary = pd.DataFrame(summary_rows)\n",
    "beta_summary\n"
   ],
   "id": "6504c2b97f996510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 6 — Posterior predictive distribution and residuals\n",
    "\n",
    "# Use posterior draws to compute fitted values and residuals\n",
    "# We'll use the posterior mean of beta for a simple fitted line,\n",
    "# and then do full posterior predictive checks below.\n",
    "\n",
    "beta_hat = beta_post.mean(axis=0)\n",
    "y_hat = X @ beta_hat\n",
    "residuals = y - y_hat\n",
    "\n",
    "plt.scatter(df[\"pct_black\"], residuals)\n",
    "plt.axhline(0.0, color=\"k\", linestyle=\"--\")\n",
    "plt.xlabel(\"Percent Black\")\n",
    "plt.ylabel(\"Residual (y - fitted)\")\n",
    "plt.title(\"Residuals vs Percent Black\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df[\"pct_hispanic\"], residuals)\n",
    "plt.axhline(0.0, color=\"k\", linestyle=\"--\")\n",
    "plt.xlabel(\"Percent Hispanic\")\n",
    "plt.ylabel(\"Residual (y - fitted)\")\n",
    "plt.title(\"Residuals vs Percent Hispanic\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df[\"polsby_popper\"], residuals)\n",
    "plt.axhline(0.0, color=\"k\", linestyle=\"--\")\n",
    "plt.xlabel(\"Polsby-Popper\")\n",
    "plt.ylabel(\"Residual (y - fitted)\")\n",
    "plt.title(\"Residuals vs Polsby-Popper\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "cb77c67bc28e5e52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 7 — Posterior predictive samples\n",
    "\n",
    "n_post = beta_post.shape[0]\n",
    "n_rep = 2000  # number of posterior samples to use for predictive checks\n",
    "idx = np.random.choice(n_post, size=n_rep, replace=False)\n",
    "\n",
    "y_rep = np.zeros((n_rep, n))\n",
    "\n",
    "for s, k in enumerate(idx):\n",
    "    beta_s = beta_post[k, :]\n",
    "    sigma_s = np.sqrt(sigma2_post[k])\n",
    "    mu_s = X @ beta_s\n",
    "    y_rep[s, :] = np.random.normal(loc=mu_s, scale=sigma_s)\n",
    "\n",
    "# Compare distributions of observed y and replicated y\n",
    "plt.hist(y, bins=10, alpha=0.7, label=\"Observed y\")\n",
    "plt.hist(y_rep.ravel(), bins=10, alpha=0.5, label=\"Posterior predictive y_rep\")\n",
    "plt.xlabel(\"Democratic share\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Posterior predictive check: Dem share\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "3b2f3c3e500f89a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 8 — Residual-like quantity from posterior predictive\n",
    "\n",
    "# Compute posterior predictive mean for each district\n",
    "y_rep_mean = y_rep.mean(axis=0)\n",
    "pp_resid = y - y_rep_mean\n",
    "\n",
    "plt.scatter(df[\"pct_black\"], pp_resid)\n",
    "plt.axhline(0.0, color=\"k\", linestyle=\"--\")\n",
    "plt.xlabel(\"Percent Black\")\n",
    "plt.ylabel(\"Posterior predictive residual\")\n",
    "plt.title(\"PP Residual vs Percent Black\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df[\"polsby_popper\"], pp_resid)\n",
    "plt.axhline(0.0, color=\"k\", linestyle=\"--\")\n",
    "plt.xlabel(\"Polsby-Popper\")\n",
    "plt.ylabel(\"Posterior predictive residual\")\n",
    "plt.title(\"PP Residual vs Polsby-Popper\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "e4fdd96d219f88b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 9 — Second-stage regression: residuals ~ compactness\n",
    "\n",
    "r = pp_resid\n",
    "Z_raw = df[[\"polsby_popper\", \"reock\", \"schwartzberg\"]].values\n",
    "Z_mean = Z_raw.mean(axis=0)\n",
    "Z_std = Z_raw.std(axis=0, ddof=1)\n",
    "Z_std[Z_std == 0] = 1.0\n",
    "Z = (Z_raw - Z_mean) / Z_std\n",
    "\n",
    "Z = np.column_stack([np.ones(n), Z])\n",
    "q = Z.shape[1]\n",
    "\n",
    "# Prior hyperparameters for gamma, tau2\n",
    "gamma0 = np.zeros(q)\n",
    "tau2_gamma = 10.0**2\n",
    "W0 = tau2_gamma * np.eye(q)\n",
    "\n",
    "alpha0_r = 2.0\n",
    "beta0_r = 1.0\n",
    "\n",
    "gamma_curr = np.zeros(q)\n",
    "tau2_curr = 1.0\n",
    "\n",
    "n_iter_r = 15000\n",
    "burn_in_r = 5000\n",
    "\n",
    "gamma_samples = np.zeros((n_iter_r, q))\n",
    "tau2_samples = np.zeros(n_iter_r)\n",
    "\n",
    "ZtZ = Z.T @ Z\n",
    "Ztr = Z.T @ r\n",
    "W0_inv = np.linalg.inv(W0)\n",
    "\n",
    "for it in range(n_iter_r):\n",
    "    # gamma | tau2, r\n",
    "    Wn_inv = ZtZ / tau2_curr + W0_inv\n",
    "    Wn = np.linalg.inv(Wn_inv)\n",
    "    gn = Wn @ (Ztr / tau2_curr + W0_inv @ gamma0)\n",
    "    gamma_curr = multivariate_normal.rvs(mean=gn, cov=Wn)\n",
    "\n",
    "    # tau2 | gamma, r\n",
    "    res_r = r - Z @ gamma_curr\n",
    "    alpha_n_r = alpha0_r + n / 2.0\n",
    "    beta_n_r = beta0_r + 0.5 * (res_r @ res_r)\n",
    "    tau2_curr = invgamma.rvs(a=alpha_n_r, scale=beta_n_r)\n",
    "\n",
    "    gamma_samples[it, :] = gamma_curr\n",
    "    tau2_samples[it] = tau2_curr\n",
    "\n",
    "gamma_post = gamma_samples[burn_in_r:, :]\n",
    "tau2_post = tau2_samples[burn_in_r:]\n",
    "\n",
    "gamma_summary = []\n",
    "for j, name in enumerate([\"intercept\", \"polsby_popper\", \"reock\", \"schwartzberg\"]):\n",
    "    row = {\n",
    "        \"parameter\": name,\n",
    "        \"mean\": gamma_post[:, j].mean(),\n",
    "        \"2.5%\": np.quantile(gamma_post[:, j], 0.025),\n",
    "        \"50%\": np.quantile(gamma_post[:, j], 0.5),\n",
    "        \"97.5%\": np.quantile(gamma_post[:, j], 0.975),\n",
    "    }\n",
    "    gamma_summary.append(row)\n",
    "\n",
    "pd.DataFrame(gamma_summary)\n"
   ],
   "id": "81c7e6485d317a27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 10 — Mixture-of-normals on dem_share\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "y = df[\"dem_share\"].values\n",
    "n = len(y)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha1 = 2.0\n",
    "alpha2 = 2.0\n",
    "\n",
    "m0 = y.mean()\n",
    "s0_sq = (y.std(ddof=1) ** 2) * 4  # fairly wide prior\n",
    "\n",
    "a0 = 2.0\n",
    "b0 = 1.0\n",
    "\n",
    "# Initialize\n",
    "z = np.random.randint(0, 2, size=n)  # 0 or 1 for two components\n",
    "lambda_curr = 0.5\n",
    "mu_curr = np.array([y.mean() - 0.05, y.mean() + 0.05])\n",
    "sigma2_curr = np.array([y.var(), y.var()])\n",
    "\n",
    "n_iter_mix = 20000\n",
    "burn_in_mix = 5000\n",
    "\n",
    "lambda_samples = np.zeros(n_iter_mix)\n",
    "mu_samples = np.zeros((n_iter_mix, 2))\n",
    "sigma2_samples_mix = np.zeros((n_iter_mix, 2))\n",
    "z_samples = np.zeros((n_iter_mix, n), dtype=int)\n",
    "\n",
    "for it in range(n_iter_mix):\n",
    "    # --- Sample z_i ---\n",
    "    for i in range(n):\n",
    "        # likelihood for each component\n",
    "        ll0 = norm.pdf(y[i], loc=mu_curr[0], scale=np.sqrt(sigma2_curr[0]))\n",
    "        ll1 = norm.pdf(y[i], loc=mu_curr[1], scale=np.sqrt(sigma2_curr[1]))\n",
    "        p0 = lambda_curr * ll0\n",
    "        p1 = (1 - lambda_curr) * ll1\n",
    "        denom = p0 + p1\n",
    "        if denom == 0:\n",
    "            p0 = p1 = 0.5\n",
    "        else:\n",
    "            p0 /= denom\n",
    "            p1 /= denom\n",
    "        z[i] = np.random.choice([0, 1], p=[p0, p1])\n",
    "\n",
    "    # --- Sample lambda ---\n",
    "    n0 = np.sum(z == 0)\n",
    "    n1 = n - n0\n",
    "    lambda_curr = np.random.beta(alpha1 + n0, alpha2 + n1)\n",
    "\n",
    "    # --- Sample (mu_k, sigma2_k) for each component ---\n",
    "    for k in [0, 1]:\n",
    "        yk = y[z == k]\n",
    "        nk = len(yk)\n",
    "        if nk > 0:\n",
    "            yk_mean = yk.mean()\n",
    "            # Posterior for sigma2_k\n",
    "            ak = a0 + nk / 2.0\n",
    "            # Use Normal-Inverse-Gamma updating\n",
    "            # Prior on mu_k: Normal(m0, s0_sq)\n",
    "            # Equivalent combined for sigma2:\n",
    "            # Sum of squared devs plus prior contribution\n",
    "            ssq = np.sum((yk - yk_mean) ** 2)\n",
    "            # For Normal-Inverse-Gamma, we can do:\n",
    "            # b_k = b0 + 0.5 * [ssq + (nk * s0_sq * (yk_mean - m0)^2) / (nk * s0_sq + s0_sq)]\n",
    "            # To keep it simple, treat sigma2 and mu as if independent w.r.t. the prior and use:\n",
    "            bk = b0 + 0.5 * ssq\n",
    "            sigma2_curr[k] = invgamma.rvs(a=ak, scale=bk)\n",
    "\n",
    "            # Posterior for mu_k given sigma2_k:\n",
    "            s2_post = 1 / (nk / sigma2_curr[k] + 1 / s0_sq)\n",
    "            m_post = s2_post * (nk * yk_mean / sigma2_curr[k] + m0 / s0_sq)\n",
    "            mu_curr[k] = np.random.normal(loc=m_post, scale=np.sqrt(s2_post))\n",
    "        else:\n",
    "            # if no data in component, just draw from prior\n",
    "            sigma2_curr[k] = invgamma.rvs(a=a0, scale=b0)\n",
    "            mu_curr[k] = np.random.normal(loc=m0, scale=np.sqrt(s0_sq))\n",
    "\n",
    "    lambda_samples[it] = lambda_curr\n",
    "    mu_samples[it, :] = mu_curr\n",
    "    sigma2_samples_mix[it, :] = sigma2_curr\n",
    "    z_samples[it, :] = z\n",
    "\n",
    "# Discard burn-in\n",
    "lambda_post = lambda_samples[burn_in_mix:]\n",
    "mu_post = mu_samples[burn_in_mix:, :]\n",
    "sigma2_post_mix = sigma2_samples_mix[burn_in_mix:, :]\n",
    "z_post = z_samples[burn_in_mix:, :]\n",
    "\n",
    "lambda_post.mean(), mu_post.mean(axis=0)\n"
   ],
   "id": "9b814613363c8a32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 11 — Summaries and posterior mixture classification\n",
    "\n",
    "lambda_mean = lambda_post.mean()\n",
    "mu_mean = mu_post.mean(axis=0)\n",
    "sigma_mean = np.sqrt(sigma2_post_mix.mean(axis=0))\n",
    "\n",
    "print(\"Posterior mean lambda:\", lambda_mean)\n",
    "print(\"Posterior mean mu0, mu1:\", mu_mean)\n",
    "print(\"Posterior mean sigma0, sigma1:\", sigma_mean)\n",
    "\n",
    "# Compute posterior probability for each district of being in component 1\n",
    "p_comp1 = (z_post == 1).mean(axis=0)  # shape: (n,)\n",
    "\n",
    "# Label each district by MAP component\n",
    "map_comp = (p_comp1 > 0.5).astype(int)\n",
    "\n",
    "df[\"mix_comp\"] = map_comp\n",
    "df[\"p_comp1\"] = p_comp1\n",
    "\n",
    "df[[\"district_id\", \"dem_share\", \"mix_comp\", \"p_comp1\"]].head()\n"
   ],
   "id": "4ee75b4732a03664"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 12 — Visual comparison of components\n",
    "\n",
    "plt.hist(y[map_comp == 0], bins=10, alpha=0.7, label=\"Component 0\")\n",
    "plt.hist(y[map_comp == 1], bins=10, alpha=0.7, label=\"Component 1\")\n",
    "plt.xlabel(\"Democratic share\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Mixture components on Dem share (MAP assignment)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "617b5ce578751351"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# CELL 13 — Regression: posterior component prob ~ race + compactness\n",
    "\n",
    "p_resp = p_comp1  # use posterior prob of being in component 1\n",
    "\n",
    "# Design matrix for this regression\n",
    "W_raw = df[[\"pct_black\", \"pct_hispanic\", \"pct_asian\", \"polsby_popper\", \"reock\", \"schwartzberg\"]].values\n",
    "W_mean = W_raw.mean(axis=0)\n",
    "W_std = W_raw.std(axis=0, ddof=1)\n",
    "W_std[W_std == 0] = 1.0\n",
    "W_stdzd = (W_raw - W_mean) / W_std\n",
    "\n",
    "W = np.column_stack([np.ones(n), W_stdzd])\n",
    "r_dim = W.shape[1]\n",
    "\n",
    "delta0 = np.zeros(r_dim)\n",
    "tau2_delta = 10.0**2\n",
    "R0 = tau2_delta * np.eye(r_dim)\n",
    "\n",
    "alpha0_p = 2.0\n",
    "beta0_p = 1.0\n",
    "\n",
    "delta_curr = np.zeros(r_dim)\n",
    "omega2_curr = 0.1\n",
    "\n",
    "n_iter_p = 15000\n",
    "burn_in_p = 5000\n",
    "\n",
    "delta_samples = np.zeros((n_iter_p, r_dim))\n",
    "omega2_samples = np.zeros(n_iter_p)\n",
    "\n",
    "WtW = W.T @ W\n",
    "Wtp = W.T @ p_resp\n",
    "R0_inv = np.linalg.inv(R0)\n",
    "\n",
    "for it in range(n_iter_p):\n",
    "    # delta | omega2, p\n",
    "    Rn_inv = WtW / omega2_curr + R0_inv\n",
    "    Rn = np.linalg.inv(Rn_inv)\n",
    "    dn = Rn @ (Wtp / omega2_curr + R0_inv @ delta0)\n",
    "    delta_curr = multivariate_normal.rvs(mean=dn, cov=Rn)\n",
    "\n",
    "    # omega2 | delta, p\n",
    "    res_p = p_resp - W @ delta_curr\n",
    "    alpha_n_p = alpha0_p + n / 2.0\n",
    "    beta_n_p = beta0_p + 0.5 * (res_p @ res_p)\n",
    "    omega2_curr = invgamma.rvs(a=alpha_n_p, scale=beta_n_p)\n",
    "\n",
    "    delta_samples[it, :] = delta_curr\n",
    "    omega2_samples[it] = omega2_curr\n",
    "\n",
    "delta_post = delta_samples[burn_in_p:, :]\n",
    "omega2_post = omega2_samples[burn_in_p:]\n",
    "\n",
    "delta_summary = []\n",
    "for j, name in enumerate([\"intercept\"] + [\"pct_black\", \"pct_hispanic\", \"pct_asian\", \"polsby_popper\", \"reock\", \"schwartzberg\"]):\n",
    "    row = {\n",
    "        \"parameter\": name,\n",
    "        \"mean\": delta_post[:, j].mean(),\n",
    "        \"2.5%\": np.quantile(delta_post[:, j], 0.025),\n",
    "        \"50%\": np.quantile(delta_post[:, j], 0.5),\n",
    "        \"97.5%\": np.quantile(delta_post[:, j], 0.975),\n",
    "    }\n",
    "    delta_summary.append(row)\n",
    "\n",
    "pd.DataFrame(delta_summary)\n"
   ],
   "id": "e73d707afb72e9d9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
