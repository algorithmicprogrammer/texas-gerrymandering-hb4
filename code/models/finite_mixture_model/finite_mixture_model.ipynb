{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Implementation of Finite Mixture Model\n",
    "\n",
    "<details>\n",
    "  <summary>Table of Contents</summary>\n",
    "  <ol>\n",
    "    <li>\n",
    "      <a href=\"#import-needed-filepaths-and-libraries\">Import Needed Filepaths and Libraries</a>\n",
    "    </li>\n",
    "    <li>\n",
    "    <a href=\"#load-dataset-into-pandas-dataframe\">\n",
    "    Load Dataset Into Pandas DataFrame\n",
    "    </a>\n",
    "    </li>\n",
    "    <li>\n",
    "    <a href=\"#define-response-variable-y\">\n",
    "    Define Response Variable y\n",
    "    </a>\n",
    "    </li>\n",
    "    <li>\n",
    "    <a href=\"#set-up-reproducible-random-number-generator\">\n",
    "    Set Up Reproducible Random Number Generator\n",
    "    </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#setting-up-mixture-model\">Setting Up Mixture Model</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#initial-parameters\">Initial Parameters</a></li>\n",
    "        <li><a href=\"#setting-priors\">Setting Priors</a></li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    <li><a href=\"#gibbs-sampler-implementation\">Gibbs Sampler Implementation</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#gibbs-sampler-updates\">Gibbs Sampler Updates</a></li>\n",
    "      </ul>\n",
    "</li>\n",
    "  </ol>\n",
    "</details>"
   ],
   "id": "ccd66e60768a4185"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Needed Filepaths and Libraries",
   "id": "f255d2bca1da6a32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from texas_gerrymandering_hb4.config import FINAL_CSV"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Dataset Into Pandas DataFrame\n",
    "Our processed dataset is read into a Pandas DataFrame."
   ],
   "id": "1a541f02f49af950"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.read_csv(FINAL_CSV)",
   "id": "ccbaa98efb2b2226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Response Variable y\n",
    "To clarify, `y` represents an array of outcomes."
   ],
   "id": "e7f0df3b9b022143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y = df[\"dem_share\"].values.astype(float)",
   "id": "6b77e33eb6958db1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set Up Reproducible Random Number Generator",
   "id": "585ba57cb68e11ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.random.seed(123)",
   "id": "8046b34ff6f628d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting Up Mixture Model",
   "id": "2e8a84b6874592cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initial Parameters\n",
    "* These parameters represent starting guesses for the Gibbs sampler.\n",
    "* `lambda` ($\\lambda$) is the mixing proportion. Starting from a place of ignorance, we assume a perfect mix between the two, and that our means are the sample means and our variances are the sample variances. $\\lambda$ is set to 0.5 to serve as a neutral initial guess.\n",
    "* `mu_1` and `mu_2` represent the initial means for each component.\n",
    "* `sigma_squared_1` and `sigma_squared_2` are the initial variances for each component."
   ],
   "id": "e0b89c9d34b110cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lambda = 0.5\n",
    "mu_1 = np.mean(y)\n",
    "mu_2 = np.mean(y)\n",
    "sigma_squared_1 = np.var(y)\n",
    "sigma_squared_2 = np.var(y)"
   ],
   "id": "bcf102058705b05b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setting Priors\n",
    "* `alpha_1` and `alpha_2` are the priors for lambda.\n",
    "$$\\lambda \\sim Beta(\\alpha_2, \\alpha_2)$$\n",
    "Because the prior for lambda is Beta(2,2), `alpha_1` and `alpha_2` are both set to 2.\n",
    "* This is a conjugate prior. Also, as a reminder, Beta(2,2) is also a Dirchilet distribution. What is beneficial about this is that the probability of obtaining 0 or 1, which is a degenerate model, is 0. Hence, as you get closer to 0 or 1, the likelihood is tiny. As a result, the problem is pushed further away from a degenerate value, so they become less likely to accidently become a point of convergence.\n",
    "* The means `m0_1` and `mu0_2` are coming from the same distribution.\n",
    "* Our variances are coming from the scaled inverse-$\\chi^2$ distribution with 2 degrees of freedom."
   ],
   "id": "4e5022eff854c381"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "alpha_1 = 2\n",
    "alpha_2 = 2\n",
    "\n",
    "mu0_1 = np.mean(y)\n",
    "mu0_2 = np.mean(y)\n",
    "\n",
    "sigma_0_squared_1 = np.var(y)\n",
    "sigma_0_squared_2 = np.var(y)"
   ],
   "id": "980c2e0dc440889f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gibbs Sampler Parameters",
   "id": "5636e5c6997b8d8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "iterations = 1000\n",
    "warmup = 500"
   ],
   "id": "d05c694656d30ce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Storage for Samples",
   "id": "ad4034fa28adb96f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lambda samples = np.zeros(iterations)\n",
    "mu1_samples = np.zeros(iterations)\n",
    "mu2_samples = np.zeros(iterations)\n",
    "sigma_squared_1_samples = np.zeros(iterations)\n",
    "sigma_squared_2_samples = np.zeros(iterations)"
   ],
   "id": "154c323de0cd0651"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaled Inverse-$\\chi^2$ Sampler",
   "id": "cee3a687b2e3bc9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gibbs Sampler Implementation\n",
    "* The process for the Gibbs sampler involves initializing parameters, iterative sampling, and continuing the iterations until convergence.\n",
    "* The benefits of the Gibbs sampler are that it handles complex, high-dimensional distributions and is easier to implement than direct sampling methods.\n",
    "\n",
    "###  Gibbs Sampler Updates\n",
    "\n",
    "#### Updating z\n",
    "* We compute the posterior probability that our $z_{i}=1$, taking into account the normal distributions that our data follows:\n",
    "$$p_{z_{i}=1} = \\frac{\\lambda_{old} \\cdot N(y_{i}|\\mu_{1,2}, \\sigma^2_{1,2})}{\\lambda_{old} \\cdot N(y_{i}|\\mu_{1,2}, \\sigma_{1,2}^2) + (1-\\lambda_{old})N(y_{i}|\\mu_{1,2}, \\sigma_{1,2}^2)}$$\n",
    "* Once the probability of $z_{i=1}$ is computed, then a new value of $z_{i}$ is computed using a binomial distribution.\n",
    "$$z_{i}^{new} \\leftarrow Bin(n, p_{z_{i}=1})$$\n",
    "\n",
    "#### Updating $\\lambda$\n",
    "* Updating $\\lambda$ is a two-step process because it follows a Beta distribution.\n",
    "* First, we update the Beta distribution using the standard form from conjugacy. Recall that our parameters are (2,2); they are being updated based on the frequency of 1's among our z's, which is why we end up with two updates for $\\alpha$ and $\\beta$. We begin updating $\\lambda$ by first updating $\\alpha$ and $\\beta$ parameters:\n",
    "$$\\alpha_{new} \\leftarrow \\alpha_{old} + \\sum z_{i}^{new}$$\n",
    "$$\\beta_{new} \\leftarrow \\beta_{old} + n -  \\sum z_{i}^{new}$$\n",
    "* We update $\\lambda$ by computing the posterior probability. Because our $\\lambda$ is coming from a Beta distribution with updated parameters, we draw a value of $\\lambda$:\n",
    "$$\\lambda_{new} \\leftarrow Beta(\\alpha_{new}, \\beta_{new})$$\n",
    "\n",
    "#### Updating Means\n",
    "* Our number of observations are $n_1 = \\sum z_{i}^{new}$ and $n_2 = \\sum z_{i}^{new}$, respectively. Recall that our $z_{i}$ values have been redrawn.\n",
    "* We examine the total number of observations, followed by the means.  We compute the means as follows:\n",
    "$$\\bar{y}_1 = \\frac{1}{n_1} \\sum y_{i} (z_{i}^{new} = 1)$$\n",
    "$$\\bar{y}_2 = \\frac{1}{n_2} \\sum y_{i} (z_{i}^{new} = 1)$$\n",
    "* Once we obtain our means, we are able to compute a posterior mean based our sample. We construct this using our initial values; we use `init` to denote this. This is combined with our prior and sample to compute a new mean as follows:\n",
    "$$\\mu_{1,new} = \\frac{\\frac{\\mu}{\\sigma_{1,init}^2} + n_1 \\frac{\\bar{y}_1}{\\sigma^2_{1, old}}}{\\frac{1}{\\sigma_{1,init}^2}}$$"
   ],
   "id": "e4d331c52d740338"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
