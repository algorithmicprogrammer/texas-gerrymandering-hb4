{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Implementation of Finite Mixture Model\n",
    "\n",
    "<details>\n",
    "  <summary>Table of Contents</summary>\n",
    "  <ol>\n",
    "    <li>\n",
    "      <a href=\"#import-needed-filepaths-and-libraries\">Import Needed Filepaths and Libraries</a>\n",
    "    </li>\n",
    "    <li>\n",
    "    <a href=\"#load-dataset-into-pandas-dataframe\">\n",
    "    Load Dataset Into Pandas DataFrame\n",
    "    </a>\n",
    "    </li>\n",
    "    <li>\n",
    "    <a href=\"#define-response-variable-y\">\n",
    "    Define Response Variable y\n",
    "    </a>\n",
    "    </li>\n",
    "    <li>\n",
    "    <a href=\"#set-up-reproducible-random-number-generator\">\n",
    "    Set Up Reproducible Random Number Generator\n",
    "    </a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#setting-up-mixture-model\">Setting Up Mixture Model</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#initial-parameters\">Initial Parameters</a></li>\n",
    "        <li><a href=\"#setting-priors\">Setting Priors</a></li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    <li><a href=\"#gibbs-sampler-implementation\">Gibbs Sampler Implementation</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#gibbs-sampler-updates\">Gibbs Sampler Updates</a></li>\n",
    "      </ul>\n",
    "</li>\n",
    "  </ol>\n",
    "</details>"
   ],
   "id": "ccd66e60768a4185"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Needed Filepaths and Libraries",
   "id": "f255d2bca1da6a32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from texas_gerrymandering_hb4.config import FINAL_CSV"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Dataset Into Pandas DataFrame\n",
    "Our processed dataset is read into a Pandas DataFrame."
   ],
   "id": "1a541f02f49af950"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df = pd.read_csv(FINAL_CSV)",
   "id": "ccbaa98efb2b2226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Response Variable `y`\n",
    "To clarify, `y` represents an array of outcomes."
   ],
   "id": "e7f0df3b9b022143"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y = df[\"dem_share\"].values.astype(float)",
   "id": "6b77e33eb6958db1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set Up Reproducible Random Number Generator",
   "id": "585ba57cb68e11ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.random.seed(123)",
   "id": "8046b34ff6f628d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setting Up Mixture Model",
   "id": "2e8a84b6874592cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initial Parameters\n",
    "* These parameters represent starting guesses for the Gibbs sampler.\n",
    "* `lambda` ($\\lambda$) is the mixing proportion. Starting from a place of ignorance, we assume a perfect mix between the two, and that our means are the sample means and our variances are the sample variances. $\\lambda$ is set to 0.5 to serve as a neutral initial guess.\n",
    "* `mu_1` and `mu_2` represent the initial means for each component.\n",
    "* `sigma_squared_1` and `sigma_squared_2` are the initial variances for each component."
   ],
   "id": "e0b89c9d34b110cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lambda = 0.5\n",
    "mu_1 = np.mean(y)\n",
    "mu_2 = np.mean(y)\n",
    "sigma_squared_1 = np.var(y)\n",
    "sigma_squared_2 = np.var(y)"
   ],
   "id": "bcf102058705b05b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setting Priors\n",
    "* `alpha_1` and `alpha_2` are the priors for lambda.\n",
    "$$\\lambda \\sim Beta(\\alpha_2, \\alpha_2)$$\n",
    "Because the prior for lambda is Beta(2,2), `alpha_1` and `alpha_2` are both set to 2.\n",
    "* This is a conjugate prior. Also, as a reminder, Beta(2,2) is also a Dirchilet distribution. What is beneficial about this is that the probability of obtaining 0 or 1, which is a degenerate model, is 0. Hence, as you get closer to 0 or 1, the likelihood is tiny. As a result, the problem is pushed further away from a degenerate value, so they become less likely to accidently become a point of convergence.\n",
    "* The means `m0_1` and `mu0_2` are coming from the same distribution.\n",
    "* Our variances are coming from the scaled inverse-$\\chi^2$ distribution with 2 degrees of freedom."
   ],
   "id": "4e5022eff854c381"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "alpha_1 = 2\n",
    "alpha_2 = 2\n",
    "\n",
    "mu0_1 = np.mean(y)\n",
    "mu0_2 = np.mean(y)\n",
    "\n",
    "sigma_0_squared_1 = np.var(y)\n",
    "sigma_0_squared_2 = np.var(y)"
   ],
   "id": "980c2e0dc440889f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gibbs Sampler Parameters",
   "id": "5636e5c6997b8d8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "iterations = 1000\n",
    "warmup = 500"
   ],
   "id": "d05c694656d30ce8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Storage for Samples",
   "id": "ad4034fa28adb96f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lambda samples = np.zeros(iterations)\n",
    "mu1_samples = np.zeros(iterations)\n",
    "mu2_samples = np.zeros(iterations)\n",
    "sigma_squared_1_samples = np.zeros(iterations)\n",
    "sigma_squared_2_samples = np.zeros(iterations)\n",
    "z_samples = np.zeros((iterations, n), dtype=init)"
   ],
   "id": "154c323de0cd0651"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaled Inverse-$\\chi^2$ Sampler",
   "id": "cee3a687b2e3bc9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rinvchisq(df, scale):\n",
    "    return df * scale / np.random.chisquare(df)\n"
   ],
   "id": "9eb402b0ca7f50ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gibbs Sampler Implementation\n",
    "* The process for the Gibbs sampler involves initializing parameters, iterative sampling, and continuing the iterations until convergence.\n",
    "* The benefits of the Gibbs sampler are that it handles complex, high-dimensional distributions and is easier to implement than direct sampling methods.\n",
    "\n",
    "###  Gibbs Sampler Updates\n",
    "\n",
    "#### Updating z\n",
    "* We compute the posterior probability that our $z_{i}=1$, taking into account the normal distributions that our data follows:\n",
    "$$p_{z_{i}=1} = \\frac{\\lambda_{old} \\cdot N(y_{i}|\\mu_{1,2}, \\sigma^2_{1,2})}{\\lambda_{old} \\cdot N(y_{i}|\\mu_{1,2}, \\sigma_{1,2}^2) + (1-\\lambda_{old})N(y_{i}|\\mu_{1,2}, \\sigma_{1,2}^2)}$$\n",
    "* Once the probability of $z_{i=1}$ is computed, then a new value of $z_{i}$ is computed using a binomial distribution.\n",
    "$$z_{i}^{new} \\leftarrow Bin(n, p_{z_{i}=1})$$\n",
    "\n",
    "#### Updating $\\lambda$\n",
    "* Updating $\\lambda$ is a two-step process because it follows a Beta distribution.\n",
    "* First, we update the Beta distribution using the standard form from conjugacy. Recall that our parameters are (2,2); they are being updated based on the frequency of 1's among our z's, which is why we end up with two updates for $\\alpha$ and $\\beta$. We begin updating $\\lambda$ by first updating $\\alpha$ and $\\beta$ parameters:\n",
    "$$\\alpha_{new} \\leftarrow \\alpha_{old} + \\sum z_{i}^{new}$$\n",
    "$$\\beta_{new} \\leftarrow \\beta_{old} + n -  \\sum z_{i}^{new}$$\n",
    "* We update $\\lambda$ by computing the posterior probability. Because our $\\lambda$ is coming from a Beta distribution with updated parameters, we draw a value of $\\lambda$:\n",
    "$$\\lambda_{new} \\leftarrow Beta(\\alpha_{new}, \\beta_{new})$$\n",
    "\n",
    "#### Updating Means\n",
    "* Our number of observations are $n_1 = \\sum z_{i}^{new}$ and $n_2 = \\sum z_{i}^{new}$, respectively. Recall that our $z_{i}$ values have been redrawn.\n",
    "* We examine the total number of observations, followed by the means.  We compute the means as follows:\n",
    "$$\\bar{y}_1 = \\frac{1}{n_1} \\sum y_{i} (z_{i}^{new} = 1)$$\n",
    "$$\\bar{y}_2 = \\frac{1}{n_2} \\sum y_{i} (z_{i}^{new} = 1)$$\n",
    "* Once we obtain our means, we are able to compute a posterior mean based our sample. We construct this using our initial values; we use `init` to denote this. This is combined with our prior and sample to compute a new mean as follows:\n",
    "$$\\mu_{1,new} = \\frac{\\frac{\\mu}{\\sigma_{1,init}^2} + n_1 \\frac{\\bar{y}_1}{\\sigma^2_{1, old}}}{\\frac{1}{\\sigma_{1,init}^2}}$$"
   ],
   "id": "e4d331c52d740338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -----------------------------\n",
    "# 3. Gibbs sampler\n",
    "# -----------------------------\n",
    "for it in range(iterations):\n",
    "    # ---- Update z (component membership for each district) ----\n",
    "    lik_1 = _lambda * norm.pdf(y, loc=mu_1, scale=np.sqrt(sigma_1_sq))\n",
    "    lik_2 = (1 - _lambda) * norm.pdf(y, loc=mu_2, scale=np.sqrt(sigma_2_sq))\n",
    "    z_probs = lik_1 / (lik_1 + lik_2)\n",
    "    z = np.random.binomial(1, z_probs)  # 1 = component 1, 0 = component 2\n",
    "\n",
    "    # ---- Update lambda ----\n",
    "    alpha_lambda_post = alpha_1 + np.sum(z)\n",
    "    beta_lambda_post = alpha_2 + (n - np.sum(z))\n",
    "    _lambda = np.random.beta(alpha_lambda_post, beta_lambda_post)\n",
    "\n",
    "    # ---- Update mu_1 ----\n",
    "    n1 = np.sum(z)\n",
    "    if n1 > 0:\n",
    "        y1_mean = np.mean(y[z == 1])\n",
    "        mu1_post_mean = ((mu0_1 / sigma0_1_sq) + n1 * y1_mean / sigma_1_sq) / \\\n",
    "                        (1 / sigma0_1_sq + n1 / sigma_1_sq)\n",
    "        mu1_post_sd = np.sqrt(1 / (1 / sigma0_1_sq + n1 / sigma_1_sq))\n",
    "    else:\n",
    "        mu1_post_mean = mu0_1\n",
    "        mu1_post_sd = np.sqrt(sigma0_1_sq)\n",
    "    mu_1 = np.random.normal(mu1_post_mean, mu1_post_sd)\n",
    "\n",
    "    # ---- Update mu_2 ----\n",
    "    n2 = n - n1\n",
    "    if n2 > 0:\n",
    "        y2_mean = np.mean(y[z == 0])\n",
    "        mu2_post_mean = ((mu0_2 / sigma0_2_sq) + n2 * y2_mean / sigma_2_sq) / \\\n",
    "                        (1 / sigma0_2_sq + n2 / sigma_2_sq)\n",
    "        mu2_post_sd = np.sqrt(1 / (1 / sigma0_2_sq + n2 / sigma_2_sq))\n",
    "    else:\n",
    "        mu2_post_mean = mu0_2\n",
    "        mu2_post_sd = np.sqrt(sigma0_2_sq)\n",
    "    mu_2 = np.random.normal(mu2_post_mean, mu2_post_sd)\n",
    "\n",
    "    # ---- Update sigma_1_sq (scaled inverse-chi-squared) ----\n",
    "    nu1_post = nu0_1 + n1\n",
    "    if n1 > 0:\n",
    "        ss1 = np.sum((y[z == 1] - mu_1) ** 2)\n",
    "    else:\n",
    "        ss1 = 0.0\n",
    "    sigma1_post_scale = (nu0_1 * sigma0_1_sq + ss1) / nu1_post\n",
    "    sigma_1_sq = rinvchisq(nu1_post, sigma1_post_scale)\n",
    "\n",
    "    # ---- Update sigma_2_sq (scaled inverse-chi-squared) ----\n",
    "    nu2_post = nu0_2 + n2\n",
    "    if n2 > 0:\n",
    "        ss2 = np.sum((y[z == 0] - mu_2) ** 2)\n",
    "    else:\n",
    "        ss2 = 0.0\n",
    "    sigma2_post_scale = (nu0_2 * sigma0_2_sq + ss2) / nu2_post\n",
    "    sigma_2_sq = rinvchisq(nu2_post, sigma2_post_scale)\n",
    "\n",
    "    # ---- Store samples ----\n",
    "    lambda_samples[it] = _lambda\n",
    "    mu1_samples[it] = mu_1\n",
    "    mu2_samples[it] = mu_2\n",
    "    sigma1_sq_samples[it] = sigma_1_sq\n",
    "    sigma2_sq_samples[it] = sigma_2_sq\n",
    "    z_samples[it, :] = z"
   ],
   "id": "119836eb3c774020"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Posterior Summaries",
   "id": "e3f6c8f02b720c9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
